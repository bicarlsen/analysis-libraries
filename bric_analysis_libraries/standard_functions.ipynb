{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Functions\n",
    "Common amongst analysis libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import logging\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.constants as phys\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df( df, path, name = 'df' ):\n",
    "    \"\"\"\n",
    "    Export a DataFrame as a .csv and .pkl.\n",
    "    \n",
    "    :param df: The DataFrame to export.\n",
    "    :param path: The path to save the files.\n",
    "    :param name: The name of the file. [Default: 'df']\n",
    "    \"\"\"\n",
    "    path = os.path.join( path, name )\n",
    "    df.to_csv( path + '.csv' )\n",
    "    df.to_pickle( path + '.pkl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_shape( file, sep = ',' ):\n",
    "    \"\"\"\n",
    "    Returns the shape of the file.\n",
    "    \n",
    "    :param file: The file to examine.\n",
    "    :param sep: The seperator between data. Can be a regular expression. [Default: ,]\n",
    "    :returns: A tuple of ( rows, columns ), where a column is deined by the given seperator.\n",
    "    \"\"\"\n",
    "    sep = re.compile( sep )\n",
    "    cols = -1\n",
    "    with open( file ) as f:\n",
    "        for rows, line in enumerate( f ):\n",
    "            r_cols = len( sep.findall( line ) )\n",
    "            if r_cols > cols:\n",
    "                cols = r_cols\n",
    "        \n",
    "    return ( rows + 1, cols )\n",
    "\n",
    "\n",
    "\n",
    "def metadata_from_file_name( \n",
    "    search, \n",
    "    file, \n",
    "    is_numeric = False,\n",
    "    decimal = 'd',\n",
    "    delimeter = '-', \n",
    "    group = 0,\n",
    "    full_path = False,\n",
    "    abs_path = False,\n",
    "    flags = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract metadata from a file name.\n",
    "    \n",
    "    :param search: A RegEx string to search for with one group to extract.\n",
    "        Delimeters are included automatically.\n",
    "        For numeric values see the <is_numeric> argument for details on how to format the regEx. \n",
    "    :param file: The file name to search in\n",
    "    :param is_numeric: Is the extracted parameter numeric?\n",
    "        Numeric values take the form of (\\d+)?(<decimal>)?(\\d+)? where \n",
    "            <decimal> is the decimal argument.\n",
    "        <search> argument should take the form of a RegEx where the special token \n",
    "            '<>' is used to indicate where the numeric part of the pattern lies.\n",
    "            (e.g. 'ex<>nm' will match ex<number>nm )\n",
    "        A preceeding 'm' will negate the value. \n",
    "        A trailing e will be used as a magnitude, where 'em' is a negative magnitude factor,\n",
    "            as in scientific notation,\n",
    "        [Default: False]\n",
    "        (e.g. 4d3e2, )\n",
    "    :param decimal: The decimal marker for numeric values. Only matters if <numeric> is True.\n",
    "        Must be a non-numeric value excluding 'e' and 'm' which are reserved characters.\n",
    "        [Default: 'd']\n",
    "    :param delimeter: The delimeter to use between data. [Default: -]\n",
    "    :param group: The match index to return. If 'all' returns all matches. [Default: 0]\n",
    "    :param full_path: Use the full file path instead of only the base name. [Default: False]\n",
    "    :param abs_path: Use the absolute file path instead of only the base name. [Default: False]\n",
    "    :param flags: Regular expression flags to use when matching. [Default: 0]\n",
    "    \n",
    "    :returns: The value of the found value, returned as int or float if numeric, string otherwise\n",
    "    :raises RuntimeError: If no match is found\n",
    "    \"\"\"\n",
    "    \n",
    "    # get file name without path or file extension\n",
    "    if abs_path:\n",
    "        file = os.path.abspath( file )\n",
    "        \n",
    "    elif full_path:\n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        file = os.path.basename( file )\n",
    "        file = os.path.splitext( file )[ 0 ]\n",
    "    \n",
    "    # modify search for delimeters and numeric types\n",
    "    # if numeric, search for preceeding 'm' and trailing exponential\n",
    "    if is_numeric:\n",
    "        search = search.replace(\n",
    "            '<>', \n",
    "            '(?<!e)(m)?(\\d+)(?:{})?(\\d+)?(?:e(m)?(\\d+))?'.format( decimal )\n",
    "        )\n",
    "        \n",
    "    \n",
    "    # use non-matching groups to match hyphen delimeter or beginning or end of string\n",
    "    start = '(?:^|(?<={}))'\n",
    "    end = '(?={}|{sep})'.format( '{}', sep = os.path.sep ) if full_path else '(?={}|$)'\n",
    "    \n",
    "    start = start.format( delimeter )\n",
    "    end   = end.format( delimeter )\n",
    "    \n",
    "    search = start + search + end\n",
    "    \n",
    "    match = re.findall( search, file, flags )\n",
    "    if len( match ) == 0:\n",
    "        # search pattern not found\n",
    "        raise RuntimeError( 'Metadata not found. Searched for {} in {}'.format( search, file ) )\n",
    "\n",
    "    if is_numeric:\n",
    "        # numeric value\n",
    "        # each element in match is a tuple of ( negative, base, decimal, negative exponent, exponent )\n",
    "        for index, parts in enumerate( match ):\n",
    "            # concat base and decimal\n",
    "            val = '{}.{}'.format( parts[ 1 ], parts[ 2 ] )\n",
    "            \n",
    "            if parts[ 0 ]:\n",
    "                # negative number\n",
    "                val = '-' + val\n",
    "            \n",
    "            if parts[ 4 ]:\n",
    "                # exponent\n",
    "                val += 'e'\n",
    "            \n",
    "                if parts[ 3 ]:\n",
    "                    # negative exponent\n",
    "                    val += '-'\n",
    "                \n",
    "                val += parts[ 4 ]\n",
    "            \n",
    "        match[ index ] = float( val )\n",
    "#             if not parts[ 4 ]:\n",
    "#                 # standard notation\n",
    "#                 val = float( parts[ 2 ] )\n",
    "\n",
    "#                 if parts[ 1 ] == decimal:\n",
    "#                     # decimal\n",
    "#                     magnitude = math.floor( math.log10( val ) ) + 1\n",
    "#                     val /= magnitude\n",
    "#                 else:\n",
    "#                     # int\n",
    "#                     val = int( val )\n",
    "\n",
    "#                 if parts[ 0 ] == 'm':\n",
    "#                     # negative\n",
    "#                     val *= -1\n",
    "\n",
    "#                 match[ index ] = val\n",
    "\n",
    "#             else:\n",
    "#                 # scientific notation\n",
    "\n",
    "#                 # decimal\n",
    "#                 if parts[ 1 ] == decimal:\n",
    "#                     # preceeding decimal\n",
    "#                     val = float( parts[ 3 ] )\n",
    "#                     magnitude = math.floor( math.log10( val ) ) + 1\n",
    "#                     val /= magnitude\n",
    "\n",
    "#                 else:\n",
    "#                     # decimal in base\n",
    "#                     try:\n",
    "#                         dec = parts[ 2 ].find( decimal )\n",
    "\n",
    "#                     except ValueError:\n",
    "#                         # zero not in base, interpret as integer\n",
    "#                         val = int( parts[ 3 ] )\n",
    "\n",
    "#                     else:\n",
    "#                         # zero in base, use as decimal\n",
    "#                         val = parts[ 2 ]\n",
    "#                         val = val[ :dec ] + '.' + val[ dec + 1: ]\n",
    "#                         val = float( val )\n",
    "\n",
    "#                     # multiply by exponent\n",
    "#                     exp = int( parts[ 4 ] )\n",
    "#                     if parts[ 3 ] == 'm':\n",
    "#                         # negative exponent\n",
    "#                         exp *= -1\n",
    "\n",
    "#                     val *= 10** exp\n",
    "\n",
    "#                 match[ index ] = float( val )\n",
    "                    \n",
    "    if group == 'all':\n",
    "        return match\n",
    "        \n",
    "    else:\n",
    "        return match[ group ]\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_metadata_values( file, metadata ):\n",
    "    \"\"\"\n",
    "    Gets metadata values from a file path.\n",
    "    \n",
    "    :param file: The file path to search.\n",
    "    :param metadata: A dictionary, keys indicate level name, values are patterns to match.\n",
    "    :returns: A dictionary of metadata values.\n",
    "    \"\"\"\n",
    "    # key is name, value is regexp pattern\n",
    "    headers = metadata.copy()\n",
    "    for name, search in metadata.items():\n",
    "        headers[ name ] = metadata_from_file_name( search, file )\n",
    "\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files( folder_path = None, file_pattern = None ):\n",
    "    \"\"\"\n",
    "    Gets files from the specified path\n",
    "\n",
    "    :param folder_path: The file path containing the data files [Default: Current Working Directory]\n",
    "    :param file_pattern: A glob pattern to filter the imported files\n",
    "    :returns: A list of file names\n",
    "    \"\"\"\n",
    "\n",
    "    if folder_path is None:\n",
    "        folder_path = os.getcwd()\n",
    "        \n",
    "    if file_pattern is None:\n",
    "        file_pattern = '*'\n",
    "    \n",
    "    return glob.glob( os.path.join( folder_path, file_pattern ) )\n",
    "\n",
    "\n",
    "def common_reindex( \n",
    "    dfs, \n",
    "    index = None, \n",
    "    how = 'linear', \n",
    "    fillna = 0, \n",
    "    add_values = None, \n",
    "    name = None,\n",
    "    threshold = None,\n",
    "    threshold_type = 'absolute'\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a common index across Pandas DataFrames.\n",
    "    Does not work for MultiIndexed DataFrames\n",
    "    \n",
    "    :param dfs: An single or iterable collection of DataFrames\n",
    "    :param index: The column to use as the index values [Default: index]\n",
    "    :param how: How to interpolate data at new index values [Default: linear]\n",
    "    :param fillna: Value to fill NaN values with [Default: 0]\n",
    "    :param add_values: A list of index values to manually add [Default: None]\n",
    "    :param name: The index name. If None uses the name of the first DataFrame. [Default: None]\n",
    "    :returns: A copy of the DataFrames reindexed as prescribed\n",
    "    \"\"\"\n",
    "    if len( dfs ) == 0:\n",
    "        return\n",
    "    \n",
    "    name = dfs[ 0 ].index.name if name is None else name\n",
    "    \n",
    "    # set index to given\n",
    "    if index is not None:\n",
    "        # TODO: MultiIndexed columns\n",
    "        dfs = [ df.copy().set_index( index ) for df in dfs ]\n",
    "        \n",
    "    # create common index\n",
    "    combined_index = [ df.index.values for df in dfs ]\n",
    "    if add_values is not None:\n",
    "        combined_index.append( add_values )\n",
    "    \n",
    "    combined_index = np.unique( np.concatenate( combined_index ) )\n",
    "    combined_index = pd.Index( combined_index, name = name )\n",
    "    \n",
    "    # reindex data\n",
    "    dfs = [ df.reindex( combined_index ).interpolate( method = how, limit_area = 'inside' ) for df in dfs ]\n",
    "    \n",
    "    if fillna is not False:\n",
    "        dfs = [ df.fillna( fillna ) for df in dfs ]\n",
    "        \n",
    "    return dfs\n",
    "\n",
    "\n",
    "\n",
    "# TODO\n",
    "def set_index_from_multicolumn( df, key, how = 'linear', fillna = 0, inplace = False ):\n",
    "    \"\"\"\n",
    "    Sets the column from a MultiIndex of a Pandas DataFrame to be the index\n",
    "    Automatically creates a common index on all found columns and interpoaltes\n",
    "    \n",
    "    :param df: The Pandas DataFrame to set the index on\n",
    "    :param key: The column key to set the new index to\n",
    "    :param how: How to interpolate data when creating the common index [Default: linear]\n",
    "    :param fillna: The value to fill NaN values with after interpolation [Default: 0]\n",
    "    :param inplace: Return a new DataFrame or replace the original [Default: False]\n",
    "    :returns: A new DataFrame in not inplace, otherwise None\n",
    "    \"\"\"\n",
    " \n",
    "    tdf = df if inplace else df.copy()\n",
    "    \n",
    "    tdf.index = tdf.xs( key, level = 'metrics', axis = 1 ).values.flatten()\n",
    "    tdf.drop( 'wavelength', level = 'metrics', axis = 1, inplace = True )\n",
    "    tdf.columns = tdf.columns.droplevel( 'metrics' )\n",
    "    \n",
    "    return ( None if inplace else tdf )\n",
    "\n",
    "\n",
    "\n",
    "def import_data( \n",
    "    import_datum, \n",
    "    folder_paths, \n",
    "    file_pattern = '*', \n",
    "    interpolate = 'linear', \n",
    "    fillna = 0,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Imports data from generic output files\n",
    "    \n",
    "    :param import_datum: The function to import a single data file\n",
    "    :param folder_path: The file path, or list of file paths containing the data files.\n",
    "    :param file_pattern: A glob pattern to filter the imported files [Default: '*']\n",
    "    :param interpolate: How to interpolate data for a common index [Default: linear]\n",
    "        Use None to prevent reindexing\n",
    "    :param fillna: Value to fill NaN values with [Default: 0]\n",
    "    :param kwargs: Additional arguments to pass to the import_datum function.\n",
    "    :returns: A Pandas DataFrame with MultiIndexed columns\n",
    "    :raises:\n",
    "    \"\"\"\n",
    "    \n",
    "    # get dataframes from files\n",
    "    if type( folder_paths ) is str:\n",
    "        # convert single folder path to list\n",
    "        folder_paths = [ folder_paths ]\n",
    "    \n",
    "    files = []\n",
    "    for folder in folder_paths:\n",
    "        files += get_files( folder, file_pattern )\n",
    "        \n",
    "    if len( files ) == 0:\n",
    "        # no files found\n",
    "        raise RuntimeError( 'No files found.' )\n",
    "        \n",
    "    df = []\n",
    "    for file in files:\n",
    "        data = import_datum( file, **kwargs ) # run local import datum function\n",
    "        df.append( data )\n",
    "        \n",
    "    if interpolate is not None:\n",
    "        df = common_reindex( df, how = interpolate, fillna = fillna )\n",
    "        \n",
    "    df = pd.concat( df, axis = 1 ).sort_index( axis = 1 )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plot_defaults():\n",
    "    \"\"\"\n",
    "    Set matplotlib plotting defautls\n",
    "    \"\"\"\n",
    "    \n",
    "    # set plot format defaults\n",
    "    mpl.rc( 'font', size = 16 )\n",
    "    mpl.rc( 'xtick', labelsize = 14 )\n",
    "    mpl.rc( 'ytick', labelsize = 14 )\n",
    "    mpl.rc( 'figure', figsize = ( 10, 8 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level_index( df, level, axis = 0 ):\n",
    "    \"\"\"\n",
    "    Returns the index of the given level name.\n",
    "    \n",
    "    :param df: The DataFrame to search.\n",
    "    :param level: The name of the level.\n",
    "    :param axis: The index axis to use. [Default: 0]\n",
    "    :returns: The index of the level.\n",
    "    \"\"\"\n",
    "    names = df.axes[ axis ].names\n",
    "    return names.index( level )\n",
    "\n",
    "\n",
    "def keep_levels( df, level = 1, axis = 1, inplace = False ):\n",
    "    \"\"\"\n",
    "    Drops outer levels of a MultiIndex, keeping the inner indices.\n",
    "    \n",
    "    :param df: the DataFrame to modify.\n",
    "    :param level: How many levels to keep. Can be an integer or a level name.\n",
    "        [Default: 1]\n",
    "    :param axis: The axis of the index to modify. [Defaut: 1]\n",
    "    :param inplace: Modify the DataFrame in place. [Default: False]\n",
    "    :returns: The modified DataFrame.\n",
    "    :raises: RuntimeError if invalid level is passed.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "        \n",
    "    if type( level ) is str:\n",
    "        # get level index of name\n",
    "        level = get_level_index( df, level, axis = axis )\n",
    "        \n",
    "    if type( axis ) is not str:\n",
    "        # get axis name\n",
    "        if axis == 0:\n",
    "            axis = 'index'\n",
    "            \n",
    "        elif axis == 1:\n",
    "            axis = 'columns'\n",
    "            \n",
    "        else:\n",
    "            # invalid axis\n",
    "            raise RuntimeError( 'Invalid axis {}.'.format( axis ) )\n",
    "    \n",
    "    \n",
    "    ax = getattr( df, axis )\n",
    "    levels = len( ax.levels )\n",
    "    if levels > level:\n",
    "        new_index = ax.droplevel( list( range( level ) ) )\n",
    "        setattr( df, axis, new_index )\n",
    "        \n",
    "    else:\n",
    "        raise RuntimeError( 'Invalid level {}'.format( level ) )\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def find_level_path( groups, key ):\n",
    "    \"\"\"\n",
    "    Returns the path of a key in a nested dictionary structure with lists as leaves\n",
    "    \n",
    "    :param groups: A nested dictionary structure with lists as leaves.\n",
    "    :param key: The key to search for in the leaves.\n",
    "    :returns: A list of the path to the found key, or False if not found.\n",
    "    \"\"\"\n",
    "    # base case\n",
    "    if type( groups ) is list:\n",
    "        if key in groups:\n",
    "            # found key\n",
    "            return []\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # traverse structure\n",
    "    else:\n",
    "        for name, child in groups.items():\n",
    "            path = find_level_path( child, key )\n",
    "            \n",
    "            if path is not False:\n",
    "                # found key\n",
    "                path.insert( 0, name )\n",
    "                return path\n",
    "            \n",
    "        # did not find key in any children    \n",
    "        return False\n",
    "    \n",
    "    \n",
    "def insert_index_levels( df, levels, names = None, key_level = 0, axis = 1 ):\n",
    "    \"\"\"\n",
    "    Insert levels into a MultIndex.\n",
    "    \n",
    "    :param df: The DataFrame to modify.\n",
    "    :param levels: List of level values.\n",
    "    :param names: List of level names. [Defualt: None]\n",
    "    \"\"\"\n",
    "    if not isinstance( levels, list ):\n",
    "        levels = [ levels ]\n",
    "    \n",
    "    if names is None:\n",
    "        names = [ None ]* len( levels )\n",
    "        \n",
    "    elif not isinstance( names, list ):\n",
    "        names = [ names ]\n",
    "    \n",
    "    # create levels\n",
    "    col_names = df.columns.values\n",
    "    \n",
    "    # convert all levels in to tuples, required for single level indexes\n",
    "    if not isinstance( col_names[ 0 ], tuple ):\n",
    "        col_names = [ ( name, ) for name in col_names ]\n",
    "\n",
    "    levels = [ \n",
    "        ( *name[ 0: key_level ], *levels, *name[ key_level: ] ) \n",
    "        for name in col_names\n",
    "    ]\n",
    "    \n",
    "    names = [ *names, *df.columns.names ]\n",
    "    \n",
    "    df.columns = pd.MultiIndex.from_tuples( levels, anmes = names )\n",
    "    return df\n",
    "    \n",
    "\n",
    "def add_index_levels( df, groups, names = None, key_level = 0, axis = 1 ):\n",
    "    \"\"\"\n",
    "    Adds addtional MultiIndex levels to a Pandas DataFrame\n",
    "    \n",
    "    :param df: The DataFrame to modify\n",
    "    :param groups: A nested dictionary with keys as the group name and \n",
    "        values a list of current level values in that group.\n",
    "        Multiple levels can be defined at once using nested dictionaries.\n",
    "        If None, all current values under key_level are added. \n",
    "    :param names: A name or list of names for the new levels. [Default: None]\n",
    "    :param key_level: The level of the current index which the grouping values exist [Default: Top Level]\n",
    "    :param axis: The axis to group. 0 for index, 1 for columns [Default: 1]\n",
    "    :returns: The grouped DataFrame\n",
    "    \"\"\"    \n",
    "    grouped = []\n",
    "    ax = df.axes[ axis ]\n",
    "    old_names = ax.names\n",
    "    names = names if ( type( names ) is list ) else [ names ]\n",
    "\n",
    "    if type( key_level ) is str:\n",
    "        key_level = names.index( key_level )\n",
    "\n",
    "    for index in ax:\n",
    "        if type( index ) is tuple:\n",
    "            key = index[ key_level ]\n",
    "        \n",
    "        else:\n",
    "            key = index\n",
    "        \n",
    "        new_index = find_level_path( groups, key )\n",
    "        if new_index is False: \n",
    "            # key not found\n",
    "            raise RuntimeError( 'Key {} not found in groups'.format( key ) )\n",
    "\n",
    "        new_index = tuple( new_index )\n",
    "        new_index += index if ( type( index ) == tuple ) else ( index, )\n",
    "\n",
    "        data = df.xs( index, axis = axis )\n",
    "        data = data.rename( new_index )\n",
    "        grouped.append( data )\n",
    "\n",
    "    grouped = pd.concat( grouped, axis = 1 )\n",
    "    \n",
    "    if names is not None:\n",
    "        grouped.columns = grouped.columns.set_names( names + old_names )\n",
    "    \n",
    "    grouped = grouped.sort_index( axis = axis )\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "\n",
    "\n",
    "def enumerate_duplicate_key( df, level = 0, axis = 1 ):\n",
    "    \"\"\"\n",
    "    If multiple keys are the same in the given index, enumerate them, making them unique\n",
    "    \n",
    "    :param df: The Pandas DataFrame to modify\n",
    "    :param level: If a MultiIndex, which level to examine [Default: Top level]\n",
    "    :param axis: The axis to examine [Default: 1]\n",
    "    :returns: A new DataFrame with enumerate indices\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    # get duplicates\n",
    "    indices = [ i for i, n in enumerate( df.columns ) if n == '1ba' ]\n",
    "    names = df.columns.values\n",
    "    if len( indices ) > 1:\n",
    "        # enumerate, starting at 1\n",
    "        names[ indices[ 1 ] ] = names[ indices[ 1 ] ] + '-{}'.format( index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataframe( path ):\n",
    "    \"\"\"\n",
    "    Creates a Pandas DataFrame from a csv file generated from the import_data() function\n",
    "    Automatically detects header columns and index\n",
    "    \n",
    "    :param path: The path to the saved dataframe\n",
    "    :returns: A Pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # get header lines\n",
    "    with open( path ) as file:\n",
    "        header_search = '^[^\\d]' # stop on digits, indicates data start\n",
    "        line = file.readline()\n",
    "        headers = 0\n",
    "        while line is not None:\n",
    "            match = re.match( header_search, line )\n",
    "            if match:\n",
    "                headers += 1\n",
    "                line = file.readline()\n",
    "                \n",
    "            else:\n",
    "                # end of headers\n",
    "                line = None \n",
    "                \n",
    "    headers = list( range( headers ) )\n",
    "    return pd.read_csv( path, header = headers, index_col = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_fit_function( fcn, param_names = None, guess = None, modify = None, **kwargs ):\n",
    "    \"\"\"\n",
    "    Returns a function that fits a pandas DataFrame to a function\n",
    "    \n",
    "    :param fcn: The function to use for fitting\n",
    "    :param param_names: Name of the parameters to use in the ultimately returned DataFrame \n",
    "        [Default: Names used in the passed function]\n",
    "    :param guess: A function used to produce the inital parameters guess\n",
    "        It should accept a Pandas Series containing the data and return a\n",
    "        tuple of the parameter predictions [Default: All 1]\n",
    "    :param modify: A function run before the fitting on the DataFrame.\n",
    "    :param kwargs: Additional parameters to be passed to scipy.optimize.curve_fit()\n",
    "    :returns: A function that accepts a Pandas DataFrame and fits the data to the provided function.\n",
    "        The function returns a Pandas DataFrame with the fit parameter and error for each parameter\n",
    "    \"\"\"\n",
    "    param_names = inspect.getfullargspec( fcn ).args[ 1: ] if param_names is None else param_names\n",
    "    \n",
    "    header = [ param_names, [ 'value', 'std' ] ]\n",
    "    header = pd.MultiIndex.from_product( header, names = [ 'parameter', 'metric' ] )\n",
    "    \n",
    "    def fitter( df ):\n",
    "        fits = pd.DataFrame( index = df.columns, columns = header )\n",
    "        mdf = df if modify is None else modify( df )\n",
    "        \n",
    "        for col in mdf:\n",
    "            data = mdf.xs( col, axis = 1 ).dropna()\n",
    "            initial = guess( data ) if callable( guess ) else guess\n",
    "            \n",
    "            try:\n",
    "                fit = curve_fit(\n",
    "                    fcn,\n",
    "                    xdata = data.index.values,\n",
    "                    ydata = data.values,\n",
    "                    p0 = initial,\n",
    "                    **kwargs\n",
    "                )\n",
    "                \n",
    "            except RuntimeError as err:\n",
    "                logging.warning( col + ': ' + err )\n",
    "                continue\n",
    "\n",
    "            # create dictionaries of parameter values and standard deviations \n",
    "            params = dict( zip( \n",
    "                [ ( param, 'value' ) for param in param_names ], \n",
    "                 fit[ 0 ] \n",
    "            ) )\n",
    "            \n",
    "            stds = dict( zip(\n",
    "                [ ( param , 'std' ) for param in param_names ],\n",
    "                np.sqrt( fit[ 1 ].diagonal() )\n",
    "            ) )\n",
    "            \n",
    "            params.update( stds )\n",
    "            fit = pd.Series( params, name = col )\n",
    "            fits.loc[ col ] = fit\n",
    "            \n",
    "        return fits\n",
    "    \n",
    "    return fitter\n",
    "\n",
    "\n",
    "\n",
    "def fits_to_df( fcn, fits, index ):\n",
    "    \"\"\"\n",
    "    Converts a Pandas DataFrame of fit parameters (output from #df_fit_function) to a\n",
    "    DataFrame of values.\n",
    "    \n",
    "    :param fcn: The function used as the fit.\n",
    "    :param fits: A DataFrame of fits, as returned from #df_fit_function).\n",
    "    :param index: The index values to use.\n",
    "    :returns: A DataFrame with each column for each row in the fits, \n",
    "        with values of the function evaluated on the provided index.\n",
    "    \"\"\"\n",
    "    for index, fit in fits.iterrows:\n",
    "        params = fit.xs( 'value', level = 'metric' )\n",
    "\n",
    "        fits = inten.xs( 'value', level = 'metric' )\n",
    "        i = pl.intensity_gaussian_population( fits.Eg0, fits.sigma, fits.t )\n",
    "\n",
    "        idata = fits.A* np.array( list( map( i, xdata ) ) )\n",
    "        inten = pd.DataFrame( idata, index = meas.index, columns = [ 'intensity' ] )\n",
    "    \n",
    "\n",
    "\n",
    "def gradient_threshold( \n",
    "    df, \n",
    "    div = 'slope', \n",
    "    threshold = -1, \n",
    "    calc = 'error',\n",
    "    window = 5\n",
    "):\n",
    "    \"\"\"\n",
    "    Thresholds data based on the local curvature.\n",
    "    \n",
    "    :param df: The DataFrame to threshold.\n",
    "    :param div: The type of derivative to examine. Use 'slope' or 'curvature'. [Default: slope]\n",
    "    :param threshold: [Default: -1]\n",
    "    :param calc: The type of threshold to use. \n",
    "        Use 'absolute' or 'error'. [Default: error]\n",
    "    :param window: Window width to calculate average gradient for error calculation.\n",
    "        [Default: 5]\n",
    "    :returns: Thresholded DataFrame\n",
    "    \"\"\"\n",
    "    def compute_grads( df ):\n",
    "        diffs = df.diff() \n",
    "    \n",
    "        # calculate x-axis differences\n",
    "        runs = diffs.index.values\n",
    "        runs = np.reshape( runs, ( runs.shape[ 0 ], 1 ) )\n",
    "        runs = np.repeat( runs, diffs.shape[ 1 ] , axis = 1 )\n",
    "        grads = diffs/ runs\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    grads = compute_grads( df )\n",
    "    \n",
    "    if div == 'curvature':\n",
    "        grads = compute_grads( grads )\n",
    "    \n",
    "    if calc == 'error':\n",
    "        # compute error threshold\n",
    "        threshold = threshold* grads.rolling( window = window ).mean().abs()\n",
    "        grads = grads.abs()\n",
    "        print( threshold )\n",
    "        df = df.where( grads < threshold )\n",
    "    \n",
    "    else:\n",
    "        # absolute threshold\n",
    "        if threshold > 0:\n",
    "            df = df.where( grads < threshold )\n",
    "\n",
    "        if threshold < 0:\n",
    "            df = df.where( grads > threshold )\n",
    "        \n",
    "    df = df.dropna()\n",
    "    df = df.reset_index( drop = True )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def break_and_align( \n",
    "    df, \n",
    "    index = None, \n",
    "    threshold = 10, \n",
    "    level = None, \n",
    "    include_index = None,\n",
    "    align_col = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Aligns breaks in the index or column exceeding threshold, creating cycles.\n",
    "    Indices with all missing data in each group are removed before splitting.\n",
    "    \n",
    "    E.g.\n",
    "    If an index has values [ 1, 2, 3, 6, 7, 9, 12, 13, 15 ] with a threshold of 2\n",
    "    The data would be broken after 3 and 9, creating three cycles:\n",
    "    [ [ 1, 2, 3 ], [ 6, 7, 9 ], [ 12, 13, 15 ] ]\n",
    "    \n",
    "    :param df: DataFrame to align.\n",
    "    :param index: The data to use as the index for splitting data.\n",
    "        For MultiIndex, must pass full column name in reference to level.\n",
    "        If None, uses the index. [Default: None]\n",
    "    :param threshold: The minimum value of a break in index values to create a new cycle.\n",
    "        [Default: 10]\n",
    "    :param level: The level to group data. If None treats each column individually.\n",
    "        [Default: None]\n",
    "    :include_index: Include index values as column with given name.\n",
    "        If None, index is called 'index'.\n",
    "        Only used if index parameter is None.\n",
    "        [Default: None]\n",
    "    :align_col: Name of the aligned column. \n",
    "        This will be the index values of each cycle shifted to start at 0.\n",
    "        If None, do not compute. [Default: None]\n",
    "    :returns: DataFrame with new column level 'cycles' of aligned data.\n",
    "    \"\"\" \n",
    "    df = df.copy()\n",
    "    \n",
    "    # column level names for re-aligned data\n",
    "    level_names = [ name for name in df.columns.names ]\n",
    "    level_names.insert( 1, 'cycle' )\n",
    "    \n",
    "    if index is None:\n",
    "        if type( include_index ) is str:\n",
    "            df.index = df.index.rename( include_index )\n",
    "\n",
    "        index_col = include_index\n",
    "      \n",
    "    cycles = []  \n",
    "    groups = df.items() if ( level is None ) else df.groupby( level = level, axis = 1 )\n",
    "    for name, data in groups:\n",
    "        if type( data ) is pd.Series:\n",
    "            data = pd.DataFrame( data )\n",
    "        \n",
    "        data = data.dropna( how = 'all' )\n",
    "        data_index = data.index if ( index is None ) else data.loc[ :, index_col ]\n",
    "        \n",
    "        breaks = np.where( # non continuous index values, before break\n",
    "            np.diff( data_index.values ) >= threshold \n",
    "        )[ 0 ] \n",
    "\n",
    "        pib = 0 # previous index break\n",
    "        for i in range( len( breaks ) ):\n",
    "            index_name = ( *name[ :-1 ], index_col )\n",
    "            \n",
    "            ib = breaks[ i ] + 1; # index break, including break point\n",
    "            cycle = data.iloc[ pib : ib ].copy() # cycle data\n",
    "            if index is None:\n",
    "                # move index values into data\n",
    "                cycle[ index_name ] = cycle.index\n",
    "            \n",
    "            if align_col is not None:\n",
    "                align_index = ( *name[ :-1 ], align_col )\n",
    "                \n",
    "                cycle_start = cycle.iloc[ 0 ][ index_name ]\n",
    "                cycle[ align_index ] =  cycle.loc[ :, index_name ].copy() - cycle_start\n",
    "                \n",
    "            # set cycle columns\n",
    "            header = [ ( head[ 0 ], i, *head[ 1: ] ) for head in cycle.columns.values ]\n",
    "            header = pd.MultiIndex.from_tuples( header, names = level_names )\n",
    "            cycle.columns = header\n",
    "            \n",
    "            cycle.reset_index( drop = True, inplace = True )\n",
    "            cycles.append( cycle )\n",
    "            pib = ib\n",
    "\n",
    "    cycles =  pd.concat( cycles, axis = 1 )\n",
    "    return cycles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------- TODO ------------------------------------\n",
    "# def furcate( df, model = std.gaussian_distribution, bins = 2, guess = None ):\n",
    "#     \"\"\"\n",
    "#     Finds values to split the data at, creating a given number of segments.\n",
    "#     Splitting values are found by creating a histogram of the data,\n",
    "#         fitting (bins - 1) models to the histogram, and choosing the\n",
    "#         intersection point of adjacent models as the splitting point.\n",
    "#     Note: Automatic guessing only works with 2 parameter models, currently \n",
    "    \n",
    "#     :param df: A one-dimensional data array.\n",
    "#     :param model: The PDF model to fit each data bin with. \n",
    "#         First parameter is center, second is spread. \n",
    "#         [Default: Gaussian]\n",
    "#     :param bins: The number of bins to create.\n",
    "#     :param guess: A function that takes in the data and outputs inital parameter guess. \n",
    "#         If None, centers are distributed equally along range, and\n",
    "#         spreads are the width of the bin.\n",
    "#         [Default: None]\n",
    "#     :returns: A list of (bins - 1) splitting points.\n",
    "#     :raises RuntimeError: If guess is None and the number of parameters for the model\n",
    "#         is not 2.\n",
    "#     \"\"\"\n",
    "#     df = df/ df.max()\n",
    "#     ( counts, edges ) = np.histogram( df, 100* bins )\n",
    "#     counts = counts/ counts.max()\n",
    "    \n",
    "#     # create inital guess if not provided\n",
    "#     if guess is None:\n",
    "#         if len( signature( model ).parameters ) != 2:\n",
    "#             raise RuntimeError( 'Can not use automatic guessing for models without 2 parameters.' )\n",
    "        \n",
    "#         centers = [ np.mean( edge[ i: i + 2 ] ) for i in range( edges.shape[ 0 ] - 1 ) ]\n",
    "#         spreads = [ ( df.max() - df.min() )/ bins ]* bins\n",
    "#         guess = list( zip( centers, spread ) )\n",
    "        \n",
    "#     # fit functions\n",
    "    \n",
    "        \n",
    "    \n",
    "#     return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxnearest( df, val ):\n",
    "    \"\"\"\n",
    "    Gets the index of the nearest value\n",
    "    \n",
    "    :param df: The Pandas DataFrame to search\n",
    "    :param val: The value to match\n",
    "    :returns: The index value of the nearest value\n",
    "    \"\"\"\n",
    "    return abs( df - val ).idxmin()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wl_to_en( l ):\n",
    "    \"\"\"\n",
    "    Converts a wavelength, given in nm, to an energy in eV\n",
    "    \n",
    "    :param l: The wavelength to convert, in nm\n",
    "    :returns: The corresponding energy in eV\n",
    "    \"\"\"\n",
    "    a = phys.physical_constants[ 'electron volt-joule relationship' ][ 0 ] # J\n",
    "    return phys.Planck* phys.c/( a* l* 1e-9 )\n",
    "\n",
    "\n",
    "def en_to_wl( e ):\n",
    "    \"\"\"\n",
    "    Converts an energy, given in eV, to a wavelength\n",
    "    \n",
    "    :param e: The energy to convert, in eV\n",
    "    :returns: The corresponding wavelength in nm\n",
    "    \"\"\"\n",
    "    a = phys.physical_constants[ 'electron volt-joule relationship' ][ 0 ] # J\n",
    "    return 1e9* phys.Planck* phys.c/( a* e )\n",
    "\n",
    "\n",
    "def gaussian_distribution( mu = 0 , sigma = 1, x = None ):\n",
    "    \"\"\"\n",
    "    A Normal or Gaussian distribution\n",
    "    \n",
    "    :param mu: The mean, or None [Default: 0]\n",
    "    :param sigma: The standard deviation, or None [Default: None]\n",
    "    \"\"\"\n",
    "    return np.exp( -np.power( ( x - mu ), 2 )/( 2* np.power( sigma, 2 ) ) )\n",
    "\n",
    "\n",
    "def boltzmann_distribution( t = 300, e = None ):\n",
    "    \"\"\"\n",
    "    The Boltzmann distribution at a given temperature\n",
    "    \n",
    "    :param t: The temperature in Kelvin, or None [Default: 300]\n",
    "    :param e: The energy in eV, or None [Default: None]\n",
    "    :returns: Returns a function describing the Boltmann distribution\n",
    "        as a function of energy and or temperature, if either are None;\n",
    "        or the value if both are not None\n",
    "    \"\"\"\n",
    "    a = phys.physical_constants[ 'electron volt-joule relationship' ][ 0 ] # J\n",
    "    k = phys.Boltzmann/ a\n",
    "    \n",
    "    if ( T is None ) and ( E is None ):\n",
    "        # function of energy and temperature\n",
    "        boltzmann = lambda E, T: np.exp( -E/( k* T ) )\n",
    "        \n",
    "    elif ( T is None ) and ( E is not None ):\n",
    "        # function of temperature only\n",
    "        boltzmann = lambda T: np.exp( -e/( k* T ) )\n",
    "        \n",
    "    elif ( T is not None ) and ( E is None ):\n",
    "        # function of energy only\n",
    "        boltzmann = lambda E: np.exp( -E/( k* t ) )\n",
    "    \n",
    "    else:\n",
    "        # both values passed, return value\n",
    "        boltzmann = np.exp( -e/( k* t ) )\n",
    "    \n",
    "    return boltzmann\n",
    "\n",
    "\n",
    "def fermi_distribution( ef = 0, t = 300, e = None ):\n",
    "    \"\"\"\n",
    "    The Fermi distribution at a given temperature and Fermi energy \n",
    "    \n",
    "    :param ef: The Fermi energy of the system in eV, or None [Default: 0]\n",
    "    :param t: The temperature at which to calculate the distribution in K, or None [Default: 300]\n",
    "    :param e: The energy at whcih to calculate the distribution in eV, or None [Default: None]\n",
    "    :returns: A function representing the Fermi distribution taking temperature and or energy\n",
    "        as parameters, or returning a value if both are specified\n",
    "    \"\"\"\n",
    "    a = phys.physical_constants[ 'electron volt-joule relationship' ][ 0 ] # J\n",
    "    k = phys.Boltzmann/ a\n",
    "    \n",
    "    if ( ef is None ) and ( t is None ) and ( e is None ):\n",
    "        # function of Ef, T, and E\n",
    "        fermi = lambda Ef, T, E: np.power( 1 + np.exp( ( E - Ef )/( k* T ) ), -1 )\n",
    "    \n",
    "    elif ( ef is None ) and ( t is None ) and ( e is not None ):\n",
    "        # function of Ef and T\n",
    "        fermi = lambda Ef, T: np.power( 1 + np.exp( ( e - Ef )/( k* T ) ), -1 )\n",
    "        \n",
    "    elif ( ef is None ) and ( t is not None ) and ( e is None ):\n",
    "        # function of Ef and E\n",
    "        fermi = lambda Ef, E: np.power( 1 + np.exp( ( E - Ef )/( k* t ) ), -1 )\n",
    "    \n",
    "    elif ( ef is None ) and ( t is not None ) and ( e is not None ):\n",
    "        # function of Ef\n",
    "        fermi = lambda Ef: np.power( 1 + np.exp( ( e - Ef )/( k* t ) ), -1 )\n",
    "    \n",
    "    elif ( ef is not None ) and ( t is None ) and ( e is None ):\n",
    "        # function of E and T\n",
    "        fermi = lambda T, E: np.power( 1 + np.exp( ( E - Ef )/( k* T ) ), -1 )\n",
    "        \n",
    "    elif ( ef is not None ) and ( t is None ) and ( e is not None ):\n",
    "        # function of T\n",
    "        fermi = lambda T: np.power( 1 + np.exp( ( e - Ef )/( k* T ) ), -1 )\n",
    "        \n",
    "    elif ( ef is not None ) and ( t is not None ) and ( e is None ):\n",
    "        # function of E\n",
    "        fermi = lambda E: np.power( 1 + np.exp( ( E - Ef )/( k* t ) ), -1 )\n",
    "    \n",
    "    else:\n",
    "        # value\n",
    "        fermi = n.power( 1 + np.exp( ( e - Ef )/( k* t ) ), -1 )\n",
    "    \n",
    "    return fermi\n",
    "\n",
    "\n",
    "def dos( e0 = 0, e = None ):\n",
    "    \"\"\"\n",
    "    The density of states\n",
    "    \n",
    "    :param e0: An energy in eV, or None [Default: 0]\n",
    "    :param e: An energy in eV, or None [Default: None]\n",
    "    :returns: A function that takes in an energy and reference energy and \n",
    "        returns the density of states function or a value\n",
    "    \"\"\"\n",
    "    \n",
    "    if ( e0 is None ) and ( e is None ):\n",
    "        # function of e and e0\n",
    "        density = lambda E, E0: np.sqrt( E - E0 )\n",
    "            \n",
    "    elif ( e0 is None ) and ( e is not None ):\n",
    "        # function of e0\n",
    "        density = lambda E0: np.sqrt( e - E0 )\n",
    "        \n",
    "    elif ( e0 is not None ) and ( e is None ):\n",
    "        # function of e\n",
    "        density = lambda E: np.sqrt( E - e0 )\n",
    "        \n",
    "    else:\n",
    "        # value\n",
    "        density = np.sqrt( e - e0 )\n",
    "        \n",
    "    return density\n",
    "    \n",
    "# TODO: Multiply functions together\n",
    "def population( ef = 0, t = 300, e0 = 0, e = None ):\n",
    "    \"\"\"\n",
    "    Returns a function or value representing the distribution of the population at a given energy\n",
    "        \n",
    "    :param ef: The Fermi energy in eV, or None [Default: 0]\n",
    "    :param t: The temperature in K, or None [Default: 300]\n",
    "    :param e0: The density of states base energy in eV, or None [ Default: 0 ]\n",
    "    :param e: The energy to evaluate the system at, or None [Default: None]\n",
    "    :returns: A function representing the population that takes as parameters \n",
    "        the values passed as None.\n",
    "        If all values are specified, returns the value directly.\n",
    "    \"\"\"\n",
    "    \n",
    "    return fermi_distribution( ef, t, e )* boltzmann_distribution( t, e )* dos( e0, e )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_levels( plot, df, show = True, level = 'metrics', axis = 1, **fig_args ):\n",
    "    \"\"\"\n",
    "    Plots each element of a Pandas DataFrame in a separate subplot.\n",
    "    \n",
    "    :param plot: A function that receives a Pandas DataSeries and axis to plot it on ( ax, data, name ).\n",
    "    :param df: The DataFrame to plot.\n",
    "    :param show: Show the plot. [Defualt: True]\n",
    "    :param level: Which level to iterate over. [Default: metrics]\n",
    "    :param axis: The axis to iterate over. [Default: 'columns']\n",
    "    :param fig_args: Keyword arguments passed to plt.subplot().\n",
    "    :returns: The Figure and Axes of the plot as a tuple ( fig, axs ).\n",
    "    \"\"\"\n",
    "    if axis == 'rows':\n",
    "        axis = 0 \n",
    "        \n",
    "    elif axis == 'columns':\n",
    "        axis = 1\n",
    "        \n",
    "    ax = df.axes[ axis ]\n",
    "    \n",
    "    levels = list( range( ax.names.index( level ) + 1 ) )\n",
    "    groups = df.groupby( level = levels, axis = axis )\n",
    "    \n",
    "    num_plots = len( groups )\n",
    "    cols = int( np.floor( np.sqrt( num_plots ) ) )\n",
    "    rows = int( np.ceil( num_plots/ cols ) )\n",
    "    fig, axs = plt.subplots( rows, cols, **fig_args )\n",
    "    index = 0\n",
    "    \n",
    "    for name, data in groups:\n",
    "        row = int( np.floor( index/ cols ) )\n",
    "        col = int( index% cols )\n",
    "        \n",
    "        if len( axs.shape ) == 1:\n",
    "            # only rows\n",
    "            ax = ax[ row ]\n",
    "            \n",
    "        else:\n",
    "            # rows and cols\n",
    "            ax = axs[ row, col ]\n",
    "        \n",
    "        plot( ax, data, name )\n",
    "        index += 1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return ( fig, axs )\n",
    "\n",
    "\n",
    "def plot_df( plot, df, show = True, **fig_args ):\n",
    "    \"\"\"\n",
    "    Plots each element of a Pandas DataFrame in a separate subplot.\n",
    "    \n",
    "    :param plot: A function that receives a Pandas DataSeries and axis to plot it on ( ax, data, name ).\n",
    "    :param df: The DataFrame to plot.\n",
    "    :param show: Show the plot. [Defualt: True]\n",
    "    :param fig_args: Keyword arguments passed to plt.subplot().\n",
    "    :returns: The Figure and Axes of the plot as a tuple ( fig, axs ).\n",
    "    \"\"\"\n",
    "    num_plots = int( df.columns.shape[ 0 ] )\n",
    "    cols = int( np.floor( np.sqrt( num_plots ) ) )\n",
    "    rows = int( np.ceil( num_plots/ cols ) )\n",
    "    fig, axs = plt.subplots( rows, cols, **fig_args )\n",
    "    index = 0\n",
    "    \n",
    "    for name, data in df.items():\n",
    "        row = int( np.floor( index/ cols ) )\n",
    "        col = int( index% cols )\n",
    "        \n",
    "        if len( axs.shape ) == 1:\n",
    "            # only rows\n",
    "            ax = ax[ row ]\n",
    "            \n",
    "        else:\n",
    "            # rows and cols\n",
    "            ax = axs[ row, col ]\n",
    "        \n",
    "        plot( ax, data, name )\n",
    "        index += 1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        return ( fig, axs )\n",
    "\n",
    "\n",
    "\n",
    "def boxplot_groups( df, groups, total = True, show = True ):\n",
    "    \"\"\"\n",
    "    Creates a box plot of a grouped Pandas Series\n",
    "    \n",
    "    :param df: A Pandas Series containing the data to be plotted\n",
    "    :param groups: A single or list of index levels to group by\n",
    "    :param total: Whether to include a plot for all data [Default: True]\n",
    "    :param show: Whether to show the plot or return the axis [Default: True]\n",
    "    :returns: None if show is True, else the matplotlib Axis it is plotted on\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots()\n",
    "    data = [ df.values ] if total else []\n",
    "    labels = [ 'All' ] if total else []\n",
    "    for name, group in df.groupby( groups ):\n",
    "        labels.append( name )\n",
    "        data.append( group.values )\n",
    "\n",
    "    axs.boxplot( data, labels = labels )\n",
    "    plt.xticks( rotation = 70 )\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        return axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
