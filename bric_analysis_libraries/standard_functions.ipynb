{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Functions\n",
    "Common amongst analysis libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import logging\n",
    "import inspect\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.constants as phys\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_df( df, path, name = 'df' ):\n",
    "    \"\"\"\n",
    "    Export a DataFrame as a .csv and .pkl.\n",
    "    \n",
    "    :param df: The DataFrame to export.\n",
    "    :param path: The path to save the files.\n",
    "    :param name: The name of the file. [Default: 'df']\n",
    "    \"\"\"\n",
    "    path = os.path.join( path, name )\n",
    "    df.to_csv( path + '.csv' )\n",
    "    df.to_pickle( path + '.pkl' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_plot_defaults():\n",
    "    \"\"\"\n",
    "    Set matplotlib plotting defautls\n",
    "    \"\"\"\n",
    "    \n",
    "    # set plot format defaults\n",
    "    mpl.rc( 'font', size = 16 )\n",
    "    mpl.rc( 'xtick', labelsize = 14 )\n",
    "    mpl.rc( 'ytick', labelsize = 14 )\n",
    "    mpl.rc( 'figure', figsize = ( 10, 8 ) )\n",
    "    \n",
    "\n",
    "def save_figure( path, kind = 'png', fig = None ):\n",
    "    \"\"\"\n",
    "    Save a figure.\n",
    "    \n",
    "    :param path: Path to save file.\n",
    "    :param kind: Format to save file. [Default: 'png']\n",
    "    :param fig: Figure to save. If None, saves current figure.\n",
    "        [Default: None]\n",
    "    \"\"\"\n",
    "    \n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "    \n",
    "    fig.savefig( path, format = kind, bbox_inches = 'tight' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_shape( file, sep = ',' ):\n",
    "    \"\"\"\n",
    "    Returns the shape of the file.\n",
    "    \n",
    "    :param file: The file to examine.\n",
    "    :param sep: The seperator between data. Can be a regular expression. [Default: ,]\n",
    "    :returns: A tuple of ( rows, columns ), where a column is deined by the given seperator.\n",
    "    \"\"\"\n",
    "    sep = re.compile( sep )\n",
    "    cols = -1\n",
    "    with open( file ) as f:\n",
    "        for rows, line in enumerate( f ):\n",
    "            r_cols = len( sep.findall( line ) )\n",
    "            if r_cols > cols:\n",
    "                cols = r_cols\n",
    "        \n",
    "    return ( rows + 1, cols )\n",
    "\n",
    "\n",
    "def metadata_from_file_name( \n",
    "    search, \n",
    "    file, \n",
    "    is_numeric = False,\n",
    "    decimal = 'd',\n",
    "    delimeter = '-', \n",
    "    group = 0,\n",
    "    full_path = False,\n",
    "    abs_path = False,\n",
    "    flags = 0\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract metadata from a file name.\n",
    "    \n",
    "    :param search: A RegEx string to search for with one group to extract.\n",
    "        Delimeters are included automatically.\n",
    "        For numeric values see the <is_numeric> argument for details on how to format the RegEx. \n",
    "    :param file: The file name to search in\n",
    "    :param is_numeric: Is the extracted parameter numeric?\n",
    "        Numeric values take the form of (\\d+)?(<decimal>)?(\\d+)? where \n",
    "            <decimal> is the decimal argument.\n",
    "        <search> argument should take the form of a RegEx where the special token \n",
    "            '<>' is used to indicate where the numeric part of the pattern lies.\n",
    "            (e.g. 'ex<>nm' will match ex<number>nm )\n",
    "        A preceeding 'm' will negate the value. \n",
    "        A trailing e will be used as a magnitude, where 'em' is a negative magnitude factor,\n",
    "            as in scientific notation,\n",
    "        [Default: False]\n",
    "        (e.g. 4d3e2, )\n",
    "    :param decimal: The decimal marker for numeric values. Only matters if <numeric> is True.\n",
    "        Must be a non-numeric value excluding 'e' and 'm' which are reserved characters.\n",
    "        [Default: 'd']\n",
    "    :param delimeter: The delimeter to use between data. [Default: -]\n",
    "    :param group: The match index to return. If 'all' returns all matches. [Default: 0]\n",
    "    :param full_path: Use the full file path instead of only the base name. [Default: False]\n",
    "    :param abs_path: Use the absolute file path instead of only the base name. [Default: False]\n",
    "    :param flags: Regular expression flags to use when matching. [Default: 0]\n",
    "    \n",
    "    :returns: The value of the found value, returned as int or float if numeric, string otherwise\n",
    "    :raises RuntimeError: If no match is found\n",
    "    \"\"\"\n",
    "    \n",
    "    # get file name without path or file extension\n",
    "    if abs_path:\n",
    "        file = os.path.abspath( file )\n",
    "        \n",
    "    elif not full_path:\n",
    "        file = os.path.basename( file )\n",
    "    \n",
    "    file = os.path.splitext( file )[ 0 ]\n",
    "    \n",
    "    # modify search for delimeters and numeric types\n",
    "    # if numeric, search for preceeding 'm' and trailing exponential\n",
    "    if is_numeric:\n",
    "        search = search.replace(\n",
    "            '<>', \n",
    "            '(?<!e)(m)?(\\d+)(?:{})?(\\d+)?(?:e(m)?(\\d+))?'.format( decimal )\n",
    "        )\n",
    "        \n",
    "    \n",
    "    # use non-matching groups to match hyphen delimeter or beginning or end of string\n",
    "    sep = os.path.sep\n",
    "    start = '(?:^|{sep}|(?<={}))'.format( '{}', sep = sep ) if full_path else '(?:^|(?<={}))'\n",
    "    end   = '(?={}|{sep}|$)'.format( '{}', sep = sep )      if full_path else '(?={}|$)'\n",
    "    \n",
    "    start = start.format( delimeter )\n",
    "    end   = end.format( delimeter )\n",
    "    \n",
    "    search = start + search + end\n",
    "    \n",
    "    match = re.findall( search, file, flags )\n",
    "    if len( match ) == 0:\n",
    "        # search pattern not found\n",
    "        raise RuntimeError( 'Metadata not found. Searched for {} in {}'.format( search, file ) )\n",
    "\n",
    "    if is_numeric:\n",
    "        # numeric value\n",
    "        # each element in match is a tuple of ( negative, base, decimal, negative exponent, exponent )\n",
    "        for index, parts in enumerate( match ):\n",
    "            # concat base and decimal\n",
    "            val = '{}.{}'.format( parts[ 1 ], parts[ 2 ] )\n",
    "            \n",
    "            if parts[ 0 ]:\n",
    "                # negative number\n",
    "                val = '-' + val\n",
    "            \n",
    "            if parts[ 4 ]:\n",
    "                # exponent\n",
    "                val += 'e'\n",
    "            \n",
    "                if parts[ 3 ]:\n",
    "                    # negative exponent\n",
    "                    val += '-'\n",
    "                \n",
    "                val += parts[ 4 ]\n",
    "            \n",
    "        match[ index ] = float( val )\n",
    "#             if not parts[ 4 ]:\n",
    "#                 # standard notation\n",
    "#                 val = float( parts[ 2 ] )\n",
    "\n",
    "#                 if parts[ 1 ] == decimal:\n",
    "#                     # decimal\n",
    "#                     magnitude = math.floor( math.log10( val ) ) + 1\n",
    "#                     val /= magnitude\n",
    "#                 else:\n",
    "#                     # int\n",
    "#                     val = int( val )\n",
    "\n",
    "#                 if parts[ 0 ] == 'm':\n",
    "#                     # negative\n",
    "#                     val *= -1\n",
    "\n",
    "#                 match[ index ] = val\n",
    "\n",
    "#             else:\n",
    "#                 # scientific notation\n",
    "\n",
    "#                 # decimal\n",
    "#                 if parts[ 1 ] == decimal:\n",
    "#                     # preceeding decimal\n",
    "#                     val = float( parts[ 3 ] )\n",
    "#                     magnitude = math.floor( math.log10( val ) ) + 1\n",
    "#                     val /= magnitude\n",
    "\n",
    "#                 else:\n",
    "#                     # decimal in base\n",
    "#                     try:\n",
    "#                         dec = parts[ 2 ].find( decimal )\n",
    "\n",
    "#                     except ValueError:\n",
    "#                         # zero not in base, interpret as integer\n",
    "#                         val = int( parts[ 3 ] )\n",
    "\n",
    "#                     else:\n",
    "#                         # zero in base, use as decimal\n",
    "#                         val = parts[ 2 ]\n",
    "#                         val = val[ :dec ] + '.' + val[ dec + 1: ]\n",
    "#                         val = float( val )\n",
    "\n",
    "#                     # multiply by exponent\n",
    "#                     exp = int( parts[ 4 ] )\n",
    "#                     if parts[ 3 ] == 'm':\n",
    "#                         # negative exponent\n",
    "#                         exp *= -1\n",
    "\n",
    "#                     val *= 10** exp\n",
    "\n",
    "#                 match[ index ] = float( val )\n",
    "                    \n",
    "    if group == 'all':\n",
    "        return match\n",
    "        \n",
    "    else:\n",
    "        return match[ group ]\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_metadata_values( file, metadata ):\n",
    "    \"\"\"\n",
    "    Gets metadata values from a file path.\n",
    "    \n",
    "    :param file: The file path to search.\n",
    "    :param metadata: A dictionary, keys indicate level name, values are patterns to match.\n",
    "    :returns: A dictionary of metadata values.\n",
    "    \"\"\"\n",
    "    # key is name, value is regexp pattern\n",
    "    headers = metadata.copy()\n",
    "    for name, search in metadata.items():\n",
    "        headers[ name ] = metadata_from_file_name( search, file )\n",
    "\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files( folder_path = None, file_pattern = None ):\n",
    "    \"\"\"\n",
    "    Gets files from the specified path\n",
    "\n",
    "    :param folder_path: The file path containing the data files [Default: Current Working Directory]\n",
    "    :param file_pattern: A glob pattern to filter the imported files\n",
    "    :returns: A list of file names\n",
    "    \"\"\"\n",
    "\n",
    "    if folder_path is None:\n",
    "        folder_path = os.getcwd()\n",
    "        \n",
    "    if file_pattern is None:\n",
    "        file_pattern = '*'\n",
    "    \n",
    "    return glob.glob( os.path.join( folder_path, file_pattern ) )\n",
    "\n",
    "\n",
    "def import_data( \n",
    "    import_datum, \n",
    "    folder_paths, \n",
    "    file_pattern = '*', \n",
    "    interpolate = 'linear', \n",
    "    fillna = 0,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Imports data from generic output files\n",
    "    \n",
    "    :param import_datum: The function to import a single data file\n",
    "    :param folder_path: The file path, or list of file paths containing the data files.\n",
    "    :param file_pattern: A glob pattern to filter the imported files [Default: '*']\n",
    "    :param interpolate: How to interpolate data for a common index [Default: linear]\n",
    "        Use None to prevent reindexing\n",
    "    :param fillna: Value to fill NaN values with [Default: 0]\n",
    "    :param kwargs: Additional arguments to pass to the import_datum function.\n",
    "    :returns: A Pandas DataFrame with MultiIndexed columns\n",
    "    :raises:\n",
    "    \"\"\"\n",
    "    \n",
    "    # get dataframes from files\n",
    "    if type( folder_paths ) is str:\n",
    "        # convert single folder path to list\n",
    "        folder_paths = [ folder_paths ]\n",
    "    \n",
    "    files = []\n",
    "    for folder in folder_paths:\n",
    "        files += get_files( folder, file_pattern )\n",
    "        \n",
    "    if len( files ) == 0:\n",
    "        # no files found\n",
    "        raise RuntimeError( 'No files found.' )\n",
    "        \n",
    "    df = []\n",
    "    for file in files:\n",
    "        data = import_datum( file, **kwargs ) # run local import datum function\n",
    "        df.append( data )\n",
    "        \n",
    "    if interpolate is not None:\n",
    "        df = common_reindex( df, how = interpolate, fillna = fillna )\n",
    "        \n",
    "    df = pd.concat( df, axis = 1 ).sort_index( axis = 1 )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample( df, method, value, how = 'linear' ):\n",
    "    \"\"\"\n",
    "    Downsamples a DataFrame.\n",
    "    \n",
    "    :param method: Method to use for down sampling.\n",
    "        + values: Down samples to the given values.\n",
    "        + samples: Down samples to the given number of samples, evenly spaced.\n",
    "        + resolution: Down samples to the given resoltuion.\n",
    "    :param value: Values associated to the down sampling method.\n",
    "    :param how: Grouping method. [Default: linear]\n",
    "    :returns: Down sampled DataFrame.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    index = df.index\n",
    "    \n",
    "    if method == 'values':\n",
    "        new_index = value\n",
    "        \n",
    "    elif method == 'samples':\n",
    "        new_index = np.linspace( index.min(), index.max(), value )\n",
    "        \n",
    "    elif method == 'resolution':\n",
    "        new_index = np.arange( index.min(), index.max() + value, value )\n",
    "    \n",
    "    # create common index\n",
    "    combined_index = [ df.index.values, new_index ]\n",
    "    combined_index = np.unique( np.concatenate( combined_index ) )\n",
    "    combined_index = pd.Index( combined_index, name = index.name )\n",
    "    \n",
    "    # reindex data\n",
    "    df = df.reindex( combined_index ).interpolate( method = how, limit_area = 'inside' )\n",
    "    df = df.reindex( new_index )\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "# TODO: Handle duplicate index values\n",
    "def common_reindex( \n",
    "    dfs, \n",
    "    index = None, \n",
    "    how = 'linear', \n",
    "    fillna = 0, \n",
    "    add_values = None, \n",
    "    name = None,\n",
    "    duplicates = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a common index across Pandas DataFrames.\n",
    "    Does not work for MultiIndexed DataFrames\n",
    "    \n",
    "    :param dfs: An single or iterable collection of DataFrames\n",
    "    :param index: The column to use as the index values [Default: index]\n",
    "    :param how: How to interpolate data at new index values [Default: linear]\n",
    "    :param fillna: Value to fill NaN values with [Default: 0]\n",
    "    :param add_values: A list of index values to manually add [Default: None]\n",
    "    :param name: The index name. If None uses the name of the first DataFrame. [Default: None]\n",
    "    :param duplicates: (Not Implemented) Function to reduce duplicate index values, or None to raise Exception.\n",
    "        [Default: None]\n",
    "    :returns: A copy of the DataFrames reindexed as prescribed\n",
    "    \"\"\"\n",
    "    if len( dfs ) == 0:\n",
    "        return\n",
    "    \n",
    "    name = dfs[ 0 ].index.name if name is None else name\n",
    "    \n",
    "    # set index to given\n",
    "    if index is not None:\n",
    "        # TODO: MultiIndexed columns\n",
    "        dfs = [ df.copy().set_index( index ) for df in dfs ]\n",
    "        \n",
    "    # create common index\n",
    "    combined_index = [ df.index.values for df in dfs ]\n",
    "    if add_values is not None:\n",
    "        combined_index.append( add_values )\n",
    "    \n",
    "    combined_index = np.unique( np.concatenate( combined_index ) )\n",
    "    combined_index = pd.Index( combined_index, name = name )\n",
    "    \n",
    "    # reindex data\n",
    "    dfs = [ df.reindex( combined_index ).interpolate( method = how, limit_area = 'inside' ) for df in dfs ]\n",
    "    \n",
    "    if fillna is not False:\n",
    "        dfs = [ df.fillna( fillna ) for df in dfs ]\n",
    "        \n",
    "    return dfs\n",
    "\n",
    "\n",
    "# TODO\n",
    "def set_index_from_multicolumn( df, key, how = 'linear', fillna = 0, inplace = False ):\n",
    "    \"\"\"\n",
    "    Sets the column from a MultiIndex of a Pandas DataFrame to be the index\n",
    "    Automatically creates a common index on all found columns and interpoaltes\n",
    "    \n",
    "    :param df: The Pandas DataFrame to set the index on\n",
    "    :param key: The column key to set the new index to\n",
    "    :param how: How to interpolate data when creating the common index [Default: linear]\n",
    "    :param fillna: The value to fill NaN values with after interpolation [Default: 0]\n",
    "    :param inplace: Return a new DataFrame or replace the original [Default: False]\n",
    "    :returns: A new DataFrame in not inplace, otherwise None\n",
    "    \"\"\"\n",
    " \n",
    "    tdf = df if inplace else df.copy()\n",
    "    \n",
    "    tdf.index = tdf.xs( key, level = 'metrics', axis = 1 ).values.flatten()\n",
    "    tdf.drop( 'wavelength', level = 'metrics', axis = 1, inplace = True )\n",
    "    tdf.columns = tdf.columns.droplevel( 'metrics' )\n",
    "    \n",
    "    return ( None if inplace else tdf )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level_index( df, level, axis = 0 ):\n",
    "    \"\"\"\n",
    "    Returns the index of the given level name.\n",
    "    \n",
    "    :param df: The DataFrame to search.\n",
    "    :param level: The name of the level.\n",
    "    :param axis: The index axis to use. [Default: 0]\n",
    "    :returns: The index of the level.\n",
    "    \"\"\"\n",
    "    names = df.axes[ axis ].names\n",
    "    return names.index( level )\n",
    "\n",
    "\n",
    "def keep_levels( df, levels, axis = 1, inplace = False ):\n",
    "    \"\"\"\n",
    "    Keeps the given levels of the index.\n",
    "    \n",
    "    :param df: the DataFrame to modify.\n",
    "    :param level: Value or list of levels to keep. Can be an integer or level names.\n",
    "        [Default: 1]\n",
    "    :param axis: The axis of the index to modify. [Defaut: 1]\n",
    "    :param inplace: Modify the DataFrame in place. [Default: False]\n",
    "    :returns: The modified DataFrame.\n",
    "    :raises ValueError: If invalid level is passed.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_vals( elm, indices ):\n",
    "        \"\"\"\n",
    "        Gets the values of an element at the given indices.\n",
    "        \n",
    "        :param elm: An iterable structure.\n",
    "        :param indices: Value or list of index values to extract.\n",
    "        :returns: Values at the given indices.\n",
    "        \"\"\"\n",
    "        t = type( elm ) # remember type\n",
    "        vals = [ elm[ ind ] for ind in indices  ] # extract values\n",
    "        \n",
    "        return t( vals ) # cast type\n",
    "        \n",
    "    \n",
    "    if type( levels ) not in ( list, tuple ):\n",
    "        levels = [ levels ]\n",
    "    \n",
    "    old = df.axes[ axis ]\n",
    "    \n",
    "    # get index values of levels\n",
    "    for index, level in enumerate( levels ):\n",
    "        if type( level ) is str:\n",
    "            levels[ index ] = get_level_index( df, level, axis = axis )\n",
    "    \n",
    "    # creat new index, keeping only desired values\n",
    "    new = pd.MultiIndex.from_tuples(\n",
    "        [ get_vals( vals, levels ) for vals in old.values ],\n",
    "        names = get_vals( old.names, levels )\n",
    "    )\n",
    "    \n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "        \n",
    "    # replace axis\n",
    "    if axis == 0:\n",
    "        df.index = new\n",
    "        \n",
    "    elif axis == 1:\n",
    "        df.columns = new\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def drop_outer_levels( df, level = 1, axis = 1, inplace = False ):\n",
    "    \"\"\"\n",
    "    Drops outer levels of a MultiIndex, keeping the inner indices.\n",
    "    \n",
    "    :param df: the DataFrame to modify.\n",
    "    :param level: How many levels to keep. Can be an integer or a level name.\n",
    "        [Default: 1]\n",
    "    :param axis: The axis of the index to modify. [Defaut: 1]\n",
    "    :param inplace: Modify the DataFrame in place. [Default: False]\n",
    "    :returns: The modified DataFrame.\n",
    "    :raises ValueError: If invalid level is passed.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "        \n",
    "    if type( level ) is str:\n",
    "        # get level index of name\n",
    "        level = get_level_index( df, level, axis = axis )\n",
    "        \n",
    "    if type( axis ) is not str:\n",
    "        # get axis name\n",
    "        if axis == 0:\n",
    "            axis = 'index'\n",
    "            \n",
    "        elif axis == 1:\n",
    "            axis = 'columns'\n",
    "            \n",
    "        else:\n",
    "            # invalid axis\n",
    "            raise RuntimeError( 'Invalid axis {}.'.format( axis ) )\n",
    "    \n",
    "    \n",
    "    ax = getattr( df, axis )\n",
    "    levels = len( ax.levels )\n",
    "    if levels > level:\n",
    "        new_index = ax.droplevel( list( range( level ) ) )\n",
    "        setattr( df, axis, new_index )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError( 'Invalid level {}'.format( level ) )\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def find_level_path( groups, key ):\n",
    "    \"\"\"\n",
    "    Returns the path of a key in a nested dictionary structure with lists as leaves\n",
    "    \n",
    "    :param groups: A nested dictionary structure with lists as leaves.\n",
    "    :param key: The key to search for in the leaves.\n",
    "    :returns: A list of the path to the found key, or False if not found.\n",
    "    \"\"\"\n",
    "    # base case\n",
    "    if type( groups ) is list:\n",
    "        if key in groups:\n",
    "            # found key\n",
    "            return []\n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    # traverse structure\n",
    "    else:\n",
    "        for name, child in groups.items():\n",
    "            path = find_level_path( child, key )\n",
    "            \n",
    "            if path is not False:\n",
    "                # found key\n",
    "                path.insert( 0, name )\n",
    "                return path\n",
    "            \n",
    "        # did not find key in any children    \n",
    "        return False\n",
    "    \n",
    "    \n",
    "def insert_index_levels( df, levels, names = None, key_level = 0, axis = 1, inplace = False  ):\n",
    "    \"\"\"\n",
    "    Insert levels into a MultiIndexed DataFrame.\n",
    "    \n",
    "    :param df: The DataFrame to modify.\n",
    "    :param levels: List of level values.\n",
    "    :param names: List of level names. [Defualt: None]\n",
    "    :param key_level: Index of insertion. [Default: 0]\n",
    "    :param axis: Axis to insert on. [Default: 1]\n",
    "    :param inplace: Transform DataFrame inplace. [Default: False]\n",
    "    :returns: Modified DataFrame.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    ax = df.axes[ axis ]\n",
    "    \n",
    "    if not isinstance( levels, list ):\n",
    "        levels = [ levels ]\n",
    "    \n",
    "    if names is None:\n",
    "        names = [ None ]* len( levels )\n",
    "        \n",
    "    elif not isinstance( names, list ):\n",
    "        names = [ names ]\n",
    "    \n",
    "    # create levels\n",
    "    col_names = ax.values\n",
    "    \n",
    "    # convert all levels in to tuples, required for single level indexes\n",
    "    if not isinstance( col_names[ 0 ], tuple ):\n",
    "        col_names = [ ( name, ) for name in col_names ]\n",
    "\n",
    "    levels = [ \n",
    "        ( *name[ 0: key_level ], *levels, *name[ key_level: ] ) \n",
    "        for name in col_names\n",
    "    ]\n",
    "    \n",
    "    level_names = ax.names\n",
    "    names = [ \n",
    "        *level_names[ :key_level ], \n",
    "        *names, \n",
    "        *level_names[ key_level: ] \n",
    "    ]\n",
    "    \n",
    "    # set index or columns\n",
    "    new_index = pd.MultiIndex.from_tuples( levels, names = names )\n",
    "    if axis == 0:\n",
    "        df.index = new_index\n",
    "    \n",
    "    elif axis == 1:\n",
    "        df.columns = new_index\n",
    "        \n",
    "    else:\n",
    "        raise ValueError( 'Invalid axis {}'.format( axis ) )\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def add_index_levels( df, groups, names = None, key_level = 0, axis = 1 ):\n",
    "    \"\"\"\n",
    "    (Not Implemented)\n",
    "    Adds addtional MultiIndex levels to a Pandas DataFrame\n",
    "    \n",
    "    :param df: The DataFrame to modify\n",
    "    :param groups: A nested dictionary with keys as the group name and \n",
    "        values a list of current level values in that group.\n",
    "        Multiple levels can be defined at once using nested dictionaries.\n",
    "        If None, all current values under key_level are added. \n",
    "    :param names: A name or list of names for the new levels. [Default: None]\n",
    "    :param key_level: The level of the current index which the grouping values exist [Default: 0]\n",
    "    :param axis: The axis to group. 0 for index, 1 for columns [Default: 1]\n",
    "    :returns: The grouped DataFrame\n",
    "    \"\"\"    \n",
    "    grouped = []\n",
    "    ax = df.axes[ axis ]\n",
    "    old_names = ax.names\n",
    "    names = names if isinstance( names, list ) else [ names ]\n",
    "\n",
    "    if isinstance( key_level, str ):\n",
    "        key_level = old_names.index( key_level )\n",
    "\n",
    "    for index in ax:\n",
    "        if isinstance( index, tuple ):\n",
    "            key = index[ key_level ]\n",
    "        \n",
    "        else:\n",
    "            key = index\n",
    "        \n",
    "        new_index = find_level_path( groups, key )\n",
    "        if new_index is False: \n",
    "            # key not found\n",
    "            raise RuntimeError( 'Key {} not found in groups'.format( key ) )\n",
    "\n",
    "        new_index = tuple( new_index )\n",
    "        new_index += index if ( type( index ) == tuple ) else ( index, )\n",
    "\n",
    "        data = df.xs( index, axis = axis )\n",
    "        data = data.rename( new_index )\n",
    "        grouped.append( data )\n",
    "\n",
    "    grouped = pd.concat( grouped, axis = 1 )\n",
    "    \n",
    "    if names is not None:\n",
    "        grouped.columns = grouped.columns.set_names( names + old_names )\n",
    "    \n",
    "    grouped = grouped.sort_index( axis = axis )\n",
    "    \n",
    "    return grouped\n",
    "\n",
    "\n",
    "\n",
    "def enumerate_duplicate_key( df, level = 0, axis = 1 ):\n",
    "    \"\"\"\n",
    "    If multiple keys are the same in the given index, enumerate them, making them unique\n",
    "    \n",
    "    :param df: The Pandas DataFrame to modify\n",
    "    :param level: If a MultiIndex, which level to examine [Default: Top level]\n",
    "    :param axis: The axis to examine [Default: 1]\n",
    "    :returns: A new DataFrame with enumerate indices\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    # get duplicates\n",
    "    indices = [ i for i, n in enumerate( df.columns ) if n == '1ba' ]\n",
    "    names = df.columns.values\n",
    "    if len( indices ) > 1:\n",
    "        # enumerate, starting at 1\n",
    "        names[ indices[ 1 ] ] = names[ indices[ 1 ] ] + '-{}'.format( index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataframe( path ):\n",
    "    \"\"\"\n",
    "    Creates a Pandas DataFrame from a csv file generated from the import_data() function\n",
    "    Automatically detects header columns and index\n",
    "    \n",
    "    :param path: The path to the saved dataframe\n",
    "    :returns: A Pandas DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    # get header lines\n",
    "    with open( path ) as file:\n",
    "        header_search = '^[^\\d]' # stop on digits, indicates data start\n",
    "        line = file.readline()\n",
    "        headers = 0\n",
    "        while line is not None:\n",
    "            match = re.match( header_search, line )\n",
    "            if match:\n",
    "                headers += 1\n",
    "                line = file.readline()\n",
    "                \n",
    "            else:\n",
    "                # end of headers\n",
    "                line = None \n",
    "                \n",
    "    headers = list( range( headers ) )\n",
    "    return pd.read_csv( path, header = headers, index_col = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_fit_function( fcn, param_names = None, guess = None, modify = None, **kwargs ):\n",
    "    \"\"\"\n",
    "    Returns a function that fits a pandas DataFrame to a function\n",
    "    \n",
    "    :param fcn: The function to use for fitting\n",
    "    :param param_names: Name of the parameters to use in the ultimately returned DataFrame \n",
    "        [Default: Names used in the passed function]\n",
    "    :param guess: A function used to produce the inital parameters guess\n",
    "        It should accept a Pandas Series containing the data and return a\n",
    "        tuple of the parameter predictions [Default: All 1]\n",
    "    :param modify: A function run before the fitting on the DataFrame.\n",
    "    :param kwargs: Additional parameters to be passed to scipy.optimize.curve_fit()\n",
    "    :returns: A function that accepts a Pandas DataFrame and fits the data to the provided function.\n",
    "        The function returns a Pandas DataFrame with the fit parameter and error for each parameter\n",
    "    \"\"\"\n",
    "    param_names = inspect.getfullargspec( fcn ).args[ 1: ] if param_names is None else param_names\n",
    "    \n",
    "    header = [ param_names, [ 'value', 'std' ] ]\n",
    "    header = pd.MultiIndex.from_product( header, names = [ 'parameter', 'metric' ] )\n",
    "    \n",
    "    def fitter( df ):\n",
    "        fits = pd.DataFrame( index = df.columns, columns = header )\n",
    "        mdf = df if modify is None else modify( df )\n",
    "        \n",
    "        for col in mdf:\n",
    "            data = mdf.xs( col, axis = 1 ).dropna()\n",
    "            initial = guess( data ) if callable( guess ) else guess\n",
    "            \n",
    "            with warnings.catch_warnings( record = True ) as w:\n",
    "                warnings.filterwarnings( 'error' )\n",
    "    \n",
    "                try:\n",
    "                    fit = curve_fit(\n",
    "                        fcn,\n",
    "                        xdata = data.index.values,\n",
    "                        ydata = data.values,\n",
    "                        p0 = initial,\n",
    "                        **kwargs\n",
    "                    )\n",
    "\n",
    "                except RuntimeError as err:\n",
    "                    logging.warning( f'{ col }: { err }' )\n",
    "                    continue\n",
    "\n",
    "                except TypeError as err:\n",
    "                    logging.warning( f'{ col }: { err }' )\n",
    "                    continue\n",
    "                    \n",
    "                except Exception as err:\n",
    "                    logging.warning( f'{ col }: { err }' )\n",
    "                    continue\n",
    "            \n",
    "            # create dictionaries of parameter values and standard deviations \n",
    "            params = dict( zip( \n",
    "                [ ( param, 'value' ) for param in param_names ], \n",
    "                 fit[ 0 ] \n",
    "            ) )\n",
    "            \n",
    "            stds = dict( zip(\n",
    "                [ ( param , 'std' ) for param in param_names ],\n",
    "                np.sqrt( fit[ 1 ].diagonal() )\n",
    "            ) )\n",
    "            \n",
    "            params.update( stds )\n",
    "            fit = pd.Series( params, name = col )\n",
    "            fits.loc[ col ] = fit\n",
    "            \n",
    "        return fits\n",
    "    \n",
    "    return fitter\n",
    "\n",
    "\n",
    "def fits_to_df( fcn, fits, index ):\n",
    "    \"\"\"\n",
    "    Converts a Pandas DataFrame of fit parameters (output from #df_fit_function) to a\n",
    "    DataFrame of values.\n",
    "    \n",
    "    :param fcn: The function used as the fit.\n",
    "    :param fits: A DataFrame of fits, as returned from #df_fit_function).\n",
    "    :param index: The index values to use.\n",
    "    :returns: A DataFrame with each column for each row in the fits, \n",
    "        with values of the function evaluated on the provided index.\n",
    "    \"\"\"\n",
    "    for index, fit in fits.iterrows:\n",
    "        params = fit.xs( 'value', level = 'metric' )\n",
    "\n",
    "        fits = inten.xs( 'value', level = 'metric' )\n",
    "        i = pl.intensity_gaussian_population( fits.Eg0, fits.sigma, fits.t )\n",
    "\n",
    "        idata = fits.A* np.array( list( map( i, xdata ) ) )\n",
    "        inten = pd.DataFrame( idata, index = meas.index, columns = [ 'intensity' ] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_mask( mask, window = 10 ):\n",
    "    \"\"\"\n",
    "    Smooths a mask of True/False values.\n",
    "    \n",
    "    :param mask: Mask to smooth.\n",
    "    :param window: Smoothing window. [Default: 10]\n",
    "    :returns: Smoothed mask.\n",
    "    \"\"\"\n",
    "    # convert False/True to 0/1\n",
    "    if isinstance( mask, pd.DataFrame ):\n",
    "        if mask.shape[ 1 ] > 1:\n",
    "            raise TypeError( 'Mask can not have more than one column.' )\n",
    "        \n",
    "        df = mask.squeeze( axis = 1 ).astype( int )\n",
    "        \n",
    "    elif isinstance( mask, pd.Series ):\n",
    "        df = mask.apply( int )\n",
    "        \n",
    "    else:\n",
    "        df = pd.Series( map( int, mask ) )\n",
    "    \n",
    "    # smooth values and convert back to False/True\n",
    "    df = df.rolling( window = window ).mean()\n",
    "    df = df.fillna( 0 )\n",
    "    df = df.apply( round )\n",
    "    df = df.apply( bool )\n",
    "    \n",
    "    return df.values\n",
    "\n",
    "\n",
    "def mask_from_threshold( \n",
    "    df, \n",
    "    threshold = 3, \n",
    "    deviation = 'std',\n",
    "    direction = 0,\n",
    "    separation = 0,\n",
    "    keep = 'first'\n",
    "):\n",
    "    \"\"\"\n",
    "    Create mask of indices breaking a threshold.\n",
    "    \n",
    "    :param df: The DataFrame to threshold.\n",
    "    :param threshold: Deviations greater than threshold are masked.\n",
    "        [Default: 3]\n",
    "    :param deviation: The type of deviation to use. \n",
    "        Values [ 'std', 'error', 'value' ].\n",
    "        'std' uses standard deviation.\n",
    "        'error' uses deviation from the mean.\n",
    "        'value' uses the raw values.\n",
    "        [Default: 'std']\n",
    "    :param direction: Direction values must pass threshold.\n",
    "        +1 for more positive, -1 for more negative, 0 for absolute.\n",
    "        [Default: 0]\n",
    "    :param separation: Minimum separation between points.\n",
    "        [Default: 0]\n",
    "    :param keep: Point to keep if multiple are within spearation of eachother,\n",
    "        or None to raise an Exception.\n",
    "        Values are [ 'first', 'last', 'middle', None ]\n",
    "        [Default: 'first']\n",
    "    :returns: Indices of values breaking threshold.\n",
    "    :raises: RuntimeException if multiple points are within separation of eachother\n",
    "        and keep is None.\n",
    "    \"\"\"\n",
    "    # setup dataframe and threshold\n",
    "    if deviation == 'std':\n",
    "        # use standard deviation\n",
    "        threshold *= df.std()\n",
    "        tdf = ( df - df.mean() ) # standardize data\n",
    "        \n",
    "    elif deviation == 'error':\n",
    "        # use error relative to mean\n",
    "        threshold *= df.mean()\n",
    "        tdf = ( df - df.mean() ) # standardize data\n",
    "\n",
    "    elif deviation == 'value':\n",
    "        # value threshold \n",
    "        tdf = df.copy()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError( 'Invalid deivation type.' )\n",
    "        \n",
    "    # get mask from direction\n",
    "    if direction == 0:\n",
    "        mask = np.where( tdf.abs() > threshold )\n",
    "        \n",
    "    elif direction == -1:\n",
    "        mask = np.where( tdf < threshold )\n",
    "        \n",
    "    elif direction == 1:\n",
    "        mask = np.where( tdf > threshold )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError( 'Invalid direction.' )\n",
    "        \n",
    "    mask = mask[ 0 ]\n",
    "\n",
    "    # check separation\n",
    "    # find mask points with separation less than specified\n",
    "    breaks = []\n",
    "    for index in range( 1, len( mask ) ):\n",
    "        if ( mask[ index ] - mask[ index - 1 ] ) > separation:\n",
    "            # index is start of new mask group\n",
    "            breaks.append( index )\n",
    "            \n",
    "    breaks.append( None ) # include final mask point\n",
    "    \n",
    "    # break mask into groups\n",
    "    pbk = 0\n",
    "    groups = []\n",
    "    for brk in breaks:\n",
    "        groups.append( mask[ pbk: brk ] )\n",
    "        pbk = brk\n",
    "    \n",
    "    # keep group points\n",
    "    if keep == 'first':\n",
    "        mask = [ group[ 0 ] for group in groups ]\n",
    "        \n",
    "    elif keep == 'last':\n",
    "        mask = [ group[ -1 ] for group in groups ]\n",
    "        \n",
    "    elif keep == 'middle':\n",
    "        mask = [ group[ int( len( group )/ 2 ) ] for group in groups ]\n",
    "            \n",
    "    elif keep is None:\n",
    "        raise RuntimeError( 'Separation violation in mask.' )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError( 'Invalid keep.' )\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def break_from_mask( df, mask, name = 'cycle', axis = 0, inplace = False ):\n",
    "    \"\"\"\n",
    "    Breaks a DataFrame into cycles, given a mask.\n",
    "    \n",
    "    :param df: A Pandas DataFrame.\n",
    "    :param mask: List of indices indicating break position.\n",
    "    :param name: Name to assign to new index. [Default: 'cycle']\n",
    "    :param axis: Axis to combine breaks. [Default: 0]\n",
    "    :param inplace: Modify DataFrame inplace. [Default: False]\n",
    "    :returns: Pandas DataFrame split into cycles.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        df = df.copy()\n",
    "    \n",
    "    ax = df.axes[ axis ]\n",
    "    names = ( name, *ax.names )\n",
    "    \n",
    "    mask = np.append( mask, None )\n",
    "    if not ( 0 in mask ): \n",
    "        mask = np.insert( mask, 0, None )\n",
    "    \n",
    "    cycles = [\n",
    "        df.iloc[ mask[ index ] : mask[ index + 1 ] ]\n",
    "        for index in range( len( mask ) - 1 )\n",
    "    ]\n",
    "    \n",
    "    # add cycle header\n",
    "    for cycle, data in enumerate( cycles ):\n",
    "        d_ax = data.axes[ axis ]\n",
    "        \n",
    "        headers = (\n",
    "            [ ( cycle, *head_val ) for head_val in d_ax.values ]\n",
    "            if isinstance( ax, pd.MultiIndex ) else\n",
    "            [ ( cycle, head_val ) for head_val in d_ax.values ]\n",
    "        )\n",
    "        \n",
    "        headers = pd.MultiIndex.from_tuples(\n",
    "            headers, names = names\n",
    "        )\n",
    "        \n",
    "        if axis == 0:\n",
    "            data.index = headers\n",
    "            \n",
    "        else:\n",
    "            data.columns = headers\n",
    "    \n",
    "    cycles = pd.concat( cycles, axis = axis )\n",
    "    return cycles\n",
    "\n",
    "\n",
    "def align_cycles( df, name = 'cycles' ):\n",
    "    \"\"\"\n",
    "    Moves cycles from columns to index, adjusting times.\n",
    "    \n",
    "    :param df: DataFrame with cycles.\n",
    "    :param name: Name of the index to align. [Default: 'cycles']\n",
    "    :returns: DataFrame with time aligned in index by scan.\n",
    "    \"\"\"\n",
    "    cycles = []\n",
    "    time = 0\n",
    "    for cycle, data in df.groupby( level = name, axis = 1 ):\n",
    "        data.index = data.index + time\n",
    "        time = data.index.max()\n",
    "\n",
    "        data = data.dropna()\n",
    "        data.columns = data.columns.droplevel( name )\n",
    "        data = std.insert_index_levels( data, cycle, name, axis = 0 )\n",
    "\n",
    "        cycles.append( data )\n",
    "\n",
    "    cycles = pd.concat( cycles, axis = 0 ).sort_index( 0 )\n",
    "    return cycles\n",
    "\n",
    "\n",
    "def gradient_threshold( \n",
    "    df, \n",
    "    div = 'slope', \n",
    "    threshold = -1, \n",
    "    calc = 'error',\n",
    "    window = 5,\n",
    "    derivative = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Thresholds data based on the local curvature.\n",
    "    \n",
    "    :param df: The DataFrame to threshold.\n",
    "    :param div: The type of derivative to examine. Use 'slope' or 'curvature'. [Default: slope]\n",
    "    :param threshold: [Default: -1]\n",
    "    :param calc: The type of threshold to use. \n",
    "        Use 'absolute' or 'error'. [Default: error]\n",
    "    :param window: Window width to calculate average gradient for error calculation.\n",
    "        [Default: 5]\n",
    "    :returns: Thresholded DataFrame\n",
    "    \"\"\"\n",
    "    def compute_grads( df ):\n",
    "        diffs = df.diff() \n",
    "    \n",
    "        # calculate x-axis differences\n",
    "        runs = diffs.index.values\n",
    "        runs = np.reshape( runs, ( runs.shape[ 0 ], 1 ) )\n",
    "        runs = np.repeat( runs, diffs.shape[ 1 ] , axis = 1 )\n",
    "        grads = diffs/ runs\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    grads = compute_grads( df )\n",
    "    \n",
    "    if div == 'curvature':\n",
    "        grads = compute_grads( grads )\n",
    "    \n",
    "    if calc == 'error':\n",
    "        # compute error threshold\n",
    "        threshold = threshold* grads.rolling( window = window ).mean().abs()\n",
    "        grads = grads.abs()\n",
    "        df = df.where( grads < threshold )\n",
    "    \n",
    "    else:\n",
    "        # absolute threshold\n",
    "        if threshold > 0:\n",
    "            df = df.where( grads < threshold )\n",
    "\n",
    "        if threshold < 0:\n",
    "            df = df.where( grads > threshold )\n",
    "        \n",
    "    df = df.dropna()\n",
    "    df = df.reset_index( drop = True )\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def break_and_align( \n",
    "    df, \n",
    "    index = None, \n",
    "    threshold = 10, \n",
    "    level = None, \n",
    "    include_index = None,\n",
    "    align_col = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Aligns breaks in the index or column exceeding threshold, creating cycles.\n",
    "    Indices with all missing data in each group are removed before splitting.\n",
    "    \n",
    "    E.g.\n",
    "    If an index has values [ 1, 2, 3, 6, 7, 9, 12, 13, 15 ] with a threshold of 2\n",
    "    The data would be broken after 3 and 9, creating three cycles:\n",
    "    [ [ 1, 2, 3 ], [ 6, 7, 9 ], [ 12, 13, 15 ] ]\n",
    "    \n",
    "    :param df: DataFrame to align.\n",
    "    :param index: The data to use as the index for splitting data.\n",
    "        For MultiIndex, must pass full column name in reference to level.\n",
    "        If None, uses the index. [Default: None]\n",
    "    :param threshold: The minimum value of a break in index values to create a new cycle.\n",
    "        [Default: 10]\n",
    "    :param level: The level to group data. If None treats each column individually.\n",
    "        [Default: None]\n",
    "    :include_index: Include index values as column with given name.\n",
    "        If None, index is called 'index'.\n",
    "        Only used if index parameter is None.\n",
    "        [Default: None]\n",
    "    :align_col: Name of the aligned column. \n",
    "        This will be the index values of each cycle shifted to start at 0.\n",
    "        If None, do not compute. [Default: None]\n",
    "    :returns: DataFrame with new column level 'cycles' of aligned data.\n",
    "    \"\"\" \n",
    "    df = df.copy()\n",
    "    \n",
    "    # column level names for re-aligned data\n",
    "    level_names = [ name for name in df.columns.names ]\n",
    "    level_names.insert( 1, 'cycle' )\n",
    "    \n",
    "    if index is None:\n",
    "        if type( include_index ) is str:\n",
    "            df.index = df.index.rename( include_index )\n",
    "\n",
    "        index_col = include_index\n",
    "      \n",
    "    cycles = []  \n",
    "    groups = df.items() if ( level is None ) else df.groupby( level = level, axis = 1 )\n",
    "    for name, data in groups:\n",
    "        if type( data ) is pd.Series:\n",
    "            data = pd.DataFrame( data )\n",
    "        \n",
    "        data = data.dropna( how = 'all' )\n",
    "        data_index = data.index if ( index is None ) else data.loc[ :, index_col ]\n",
    "        \n",
    "        breaks = np.where( # non continuous index values, before break\n",
    "            np.diff( data_index.values ) >= threshold \n",
    "        )[ 0 ] \n",
    "\n",
    "        pib = 0 # previous index break\n",
    "        for i in range( len( breaks ) ):\n",
    "            index_name = ( *name[ :-1 ], index_col )\n",
    "            \n",
    "            ib = breaks[ i ] + 1; # index break, including break point\n",
    "            cycle = data.iloc[ pib : ib ].copy() # cycle data\n",
    "            if index is None:\n",
    "                # move index values into data\n",
    "                cycle[ index_name ] = cycle.index\n",
    "            \n",
    "            if align_col is not None:\n",
    "                align_index = ( *name[ :-1 ], align_col )\n",
    "                \n",
    "                cycle_start = cycle.iloc[ 0 ][ index_name ]\n",
    "                cycle[ align_index ] =  cycle.loc[ :, index_name ].copy() - cycle_start\n",
    "                \n",
    "            # set cycle columns\n",
    "            header = [ ( head[ 0 ], i, *head[ 1: ] ) for head in cycle.columns.values ]\n",
    "            header = pd.MultiIndex.from_tuples( header, names = level_names )\n",
    "            cycle.columns = header\n",
    "            \n",
    "            cycle.reset_index( drop = True, inplace = True )\n",
    "            cycles.append( cycle )\n",
    "            pib = ib\n",
    "\n",
    "    cycles = pd.concat( cycles, axis = 1 )\n",
    "    return cycles\n",
    "\n",
    "\n",
    "def break_from_gradient( \n",
    "    df, \n",
    "    threshold = -1, \n",
    "    calc = 'error',\n",
    "    derivative = 1,\n",
    "    window = 5,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Thresholds data based on the local curvature.\n",
    "    \n",
    "    :param df: The DataFrame to threshold.\n",
    "    :param div: The type of derivative to examine. Use 'slope' or 'curvature'. [Default: slope]\n",
    "    :param threshold: [Default: -1]\n",
    "    :param calc: The type of threshold to use. \n",
    "        Use 'absolute' or 'error'. [Default: error]\n",
    "    :param derivative: Number of derivatives to compute. [Default: 1]\n",
    "    :param window: Window width to calculate average gradient for error calculation.\n",
    "        [Default: 5]\n",
    "    :param kwargs: Parameters passed to #break_from_mask.\n",
    "    :returns: Thresholded DataFrame\n",
    "    \"\"\"\n",
    "    def compute_grads( df ):\n",
    "        diffs = df.diff() \n",
    "    \n",
    "        # calculate x-axis differences\n",
    "        runs = diffs.index.values\n",
    "        runs = np.reshape( runs, ( runs.shape[ 0 ], 1 ) )\n",
    "        runs = np.repeat( runs, diffs.shape[ 1 ] , axis = 1 )\n",
    "        grads = diffs/ runs\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    \n",
    "    grads = df\n",
    "    for _ in range( derivative ):\n",
    "        grads = compute_grads( grads )\n",
    "    \n",
    "    if calc == 'error':\n",
    "        # compute error threshold\n",
    "        threshold = threshold* grads.rolling( window = window ).mean().abs()\n",
    "        grads = grads.abs()\n",
    "        mask = df.where( grads > threshold )\n",
    "    \n",
    "    else:\n",
    "        # absolute threshold\n",
    "        if threshold > 0:\n",
    "            mask = df.where( grads > threshold )\n",
    "\n",
    "        if threshold < 0:\n",
    "            mask = df.where( grads < threshold )\n",
    "        \n",
    "    df = break_from_mask( df, mask, **kwargs )\n",
    "    return df\n",
    "\n",
    "\n",
    "#-------------------------------------- TODO ------------------------------------\n",
    "# def furcate( df, model = std.gaussian_distribution, bins = 2, guess = None ):\n",
    "#     \"\"\"\n",
    "#     Finds values to split the data at, creating a given number of segments.\n",
    "#     Splitting values are found by creating a histogram of the data,\n",
    "#         fitting (bins - 1) models to the histogram, and choosing the\n",
    "#         intersection point of adjacent models as the splitting point.\n",
    "#     Note: Automatic guessing only works with 2 parameter models, currently \n",
    "    \n",
    "#     :param df: A one-dimensional data array.\n",
    "#     :param model: The PDF model to fit each data bin with. \n",
    "#         First parameter is center, second is spread. \n",
    "#         [Default: Gaussian]\n",
    "#     :param bins: The number of bins to create.\n",
    "#     :param guess: A function that takes in the data and outputs inital parameter guess. \n",
    "#         If None, centers are distributed equally along range, and\n",
    "#         spreads are the width of the bin.\n",
    "#         [Default: None]\n",
    "#     :returns: A list of (bins - 1) splitting points.\n",
    "#     :raises RuntimeError: If guess is None and the number of parameters for the model\n",
    "#         is not 2.\n",
    "#     \"\"\"\n",
    "#     df = df/ df.max()\n",
    "#     ( counts, edges ) = np.histogram( df, 100* bins )\n",
    "#     counts = counts/ counts.max()\n",
    "    \n",
    "#     # create inital guess if not provided\n",
    "#     if guess is None:\n",
    "#         if len( signature( model ).parameters ) != 2:\n",
    "#             raise RuntimeError( 'Can not use automatic guessing for models without 2 parameters.' )\n",
    "        \n",
    "#         centers = [ np.mean( edge[ i: i + 2 ] ) for i in range( edges.shape[ 0 ] - 1 ) ]\n",
    "#         spreads = [ ( df.max() - df.min() )/ bins ]* bins\n",
    "#         guess = list( zip( centers, spread ) )\n",
    "        \n",
    "#     # fit functions\n",
    "    \n",
    "        \n",
    "    \n",
    "#     return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxnearest( df, val ):\n",
    "    \"\"\"\n",
    "    Gets the index of the nearest value\n",
    "    \n",
    "    :param df: The Pandas DataFrame to search\n",
    "    :param val: The value to match\n",
    "    :returns: The index value of the nearest value\n",
    "    \"\"\"\n",
    "    return abs( df - val ).idxmin()\n",
    "    \n",
    "    \n",
    "def df_grad( series ):\n",
    "    \"\"\"\n",
    "    Gradient function for use of DataFrame#apply.\n",
    "    \"\"\"\n",
    "    return np.gradient( series.values, series.index.values )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_from_counter( counter, rows, cols ):\n",
    "    \"\"\"\n",
    "    Get the row and column of a matrix form a counter.\n",
    "    \n",
    "    :param counter: Counter.\n",
    "    :param rows: Number of rows in matrix.\n",
    "    :param cols: Number of columns in matrix.\n",
    "    :returns: ( row, column ) of counter.\n",
    "    \"\"\"\n",
    "    \n",
    "    row = int( np.floor( counter / cols ) )\n",
    "    col = int( counter % cols )\n",
    "    \n",
    "    return ( row, col )\n",
    "\n",
    "\n",
    "def ax_from_counter( counter, axs ):\n",
    "    \"\"\"\n",
    "    Gets an axis from an array of axes based on a counter.\n",
    "    \n",
    "    :param counter: Counter.\n",
    "    :param axs: Matrix of axes.\n",
    "    :returns: Axis.\n",
    "    \"\"\"\n",
    "    row, col = index_from_counter( counter, *axs.shape )\n",
    "        \n",
    "    if len( axs.shape ) == 1:\n",
    "        # only rows\n",
    "        ax = ax[ row ]\n",
    "\n",
    "    else:\n",
    "        # rows and cols\n",
    "        ax = axs[ row, col ]\n",
    "        \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_levels( plot, df, show = True, level = 'metrics', axis = 1, **fig_args ):\n",
    "    \"\"\"\n",
    "    Plots each element of a Pandas DataFrame in a separate subplot.\n",
    "    \n",
    "    :param plot: A function that receives a Pandas DataSeries and axis to plot it on ( ax, data, name ).\n",
    "    :param df: The DataFrame to plot.\n",
    "    :param show: Show the plot. [Defualt: True]\n",
    "    :param level: Which level to iterate over. [Default: 'metrics']\n",
    "    :param axis: The axis to iterate over. [Default: 'columns']\n",
    "    :param fig_args: Keyword arguments passed to plt.subplot().\n",
    "    :returns: The Figure and Axes of the plot as a tuple ( fig, axs ).\n",
    "    \"\"\"\n",
    "    if axis == 'rows':\n",
    "        axis = 0 \n",
    "        \n",
    "    elif axis == 'columns':\n",
    "        axis = 1\n",
    "        \n",
    "    ax = df.axes[ axis ]\n",
    "    \n",
    "    levels = list( range( ax.names.index( level ) + 1 ) )\n",
    "    groups = df.groupby( level = levels, axis = axis )\n",
    "    \n",
    "    num_plots = len( groups )\n",
    "    cols = int( np.floor( np.sqrt( num_plots ) ) )\n",
    "    rows = int( np.ceil( num_plots/ cols ) )\n",
    "    fig, axs = plt.subplots( rows, cols, **fig_args )\n",
    "    index = 0\n",
    "    \n",
    "    for name, data in groups:\n",
    "        ax = ax_from_counter( index, axs )\n",
    "        plot( ax, data, name )\n",
    "        index += 1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    return ( fig, axs )\n",
    "\n",
    "\n",
    "def plot_df( plot, df, show = True, **fig_args ):\n",
    "    \"\"\"\n",
    "    Plots each element of a Pandas DataFrame in a separate subplot.\n",
    "    \n",
    "    :param plot: A function that receives a Pandas DataSeries and axis to plot it on ( ax, data, name ).\n",
    "    :param df: The DataFrame to plot.\n",
    "    :param show: Show the plot. [Defualt: True]\n",
    "    :param fig_args: Keyword arguments passed to plt.subplot().\n",
    "    :returns: The Figure and Axes of the plot as a tuple ( fig, axs ).\n",
    "    \"\"\"\n",
    "    num_plots = int( df.columns.shape[ 0 ] )\n",
    "    cols = int( np.floor( np.sqrt( num_plots ) ) )\n",
    "    rows = int( np.ceil( num_plots/ cols ) )\n",
    "    fig, axs = plt.subplots( rows, cols, **fig_args )\n",
    "    index = 0\n",
    "    \n",
    "    for name, data in df.items():\n",
    "        row = int( np.floor( index/ cols ) )\n",
    "        col = int( index% cols )\n",
    "        \n",
    "        if len( axs.shape ) == 1:\n",
    "            # only rows\n",
    "            ax = ax[ row ]\n",
    "            \n",
    "        else:\n",
    "            # rows and cols\n",
    "            ax = axs[ row, col ]\n",
    "        \n",
    "        plot( ax, data, name )\n",
    "        index += 1\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        return ( fig, axs )\n",
    "\n",
    "\n",
    "\n",
    "def boxplot_groups( df, groups, total = True, show = True ):\n",
    "    \"\"\"\n",
    "    Creates a box plot of a grouped Pandas Series\n",
    "    \n",
    "    :param df: A Pandas Series containing the data to be plotted\n",
    "    :param groups: A single or list of index levels to group by\n",
    "    :param total: Whether to include a plot for all data [Default: True]\n",
    "    :param show: Whether to show the plot or return the axis [Default: True]\n",
    "    :returns: None if show is True, else the matplotlib Axis it is plotted on\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots()\n",
    "    data = [ df.values ] if total else []\n",
    "    labels = [ 'All' ] if total else []\n",
    "    for name, group in df.groupby( groups ):\n",
    "        labels.append( name )\n",
    "        data.append( group.values )\n",
    "\n",
    "    axs.boxplot( data, labels = labels )\n",
    "    plt.xticks( rotation = 70 )\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        return axs\n",
    "    \n",
    "    \n",
    "def temperature_plot_rainbow( df, colorbar = True, **kwargs ):\n",
    "    \"\"\"\n",
    "    Plots a DataFrame by temperature.\n",
    "    \n",
    "    :param df: A DataFrame with temperature as the first colum index level.\n",
    "    :param colorbar: Whether to include the color bar legend. [Default: True]\n",
    "    :param **kwargs: Arguments passed to pandas.DataFrame#plot\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    NUM_COLORS = df.shape[ 1 ]\n",
    "    cm = plt.get_cmap( 'gist_rainbow' )\n",
    "    ax.set_prop_cycle( color = [ cm( float( i / NUM_COLORS ) ) for i in range( NUM_COLORS ) ] )\n",
    "\n",
    "    df.plot( ax = ax, **kwargs )\n",
    "    \n",
    "    if colorbar:\n",
    "        temp_vals = df.columns.get_level_values( 0 ).values\n",
    "        cax = fig.add_axes( [0.92, 0.15, 0.05, 0.7] )\n",
    "        cbar = mpl.colorbar.ColorbarBase( \n",
    "            ax = cax, \n",
    "            cmap = cm,  \n",
    "            norm = mpl.colors.Normalize( vmin = temp_vals.min(), vmax = temp_vals.max() ),\n",
    "            orientation = 'vertical'\n",
    "        )\n",
    "        \n",
    "        cbar_label = df.columns.names[ 0 ]\n",
    "        cbar.set_label( cbar_label, labelpad = 15 )\n",
    "    \n",
    "    return ( fig, ax )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
