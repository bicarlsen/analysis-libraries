{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EC Lab Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import math\n",
    "import logging\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bric_analysis_libraries import standard_functions as std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def get_channel_from_file_name( file, prefix = 'C' ):\n",
    "    \"\"\"\n",
    "    :param prefix: The prefix of the channel. [Default: 'C']\n",
    "    \"\"\"\n",
    "    channel_search = '{}<>'.format( prefix )\n",
    "    return std.metadata_from_file_name( \n",
    "        channel_search, \n",
    "        file, \n",
    "        is_numeric = True, \n",
    "        delimeter = '',\n",
    "        abs_path = True\n",
    "    )\n",
    "\n",
    "\n",
    "def get_holder_from_file_name( file ):\n",
    "    holder_search = 'holder\\-<>'\n",
    "    return std.metadata_from_file_name( \n",
    "        holder_search, \n",
    "        file, \n",
    "        is_numeric = True, \n",
    "        delimeter = '',\n",
    "        abs_path = True\n",
    "    )\n",
    "\n",
    "\n",
    "def get_program_from_file_name( file ):\n",
    "    program_search = '<>'\n",
    "    return std.metadata_from_file_name(\n",
    "        program_search,\n",
    "        file,\n",
    "        is_numeric = True,\n",
    "        delimeter = '_'\n",
    "    )\n",
    "\n",
    "\n",
    "def metadata_from_file_name( file, metadata, channel_prefix = 'C' ):\n",
    "    \"\"\"\n",
    "   Returns a dictionary of metadata values\n",
    "    \n",
    "    :param metadata: A list of keywords of metadata to use for indexing. \n",
    "        Values: [ 'holder', 'channel', 'program' ]\n",
    "        Default: [ 'holder', 'channel', 'program' ]\n",
    "    :param channel_prefix: The prefix for matching channel metadata. [Defualt: 'C']\n",
    "    :return: A dictionary of metadata values\n",
    "    \"\"\"\n",
    "    # create multiindex from metadata\n",
    "    header_vals  = {}\n",
    "\n",
    "    if 'channel' in metadata:\n",
    "        header_vals[ 'channel' ] = int( get_channel_from_file_name( file, prefix = channel_prefix ) )\n",
    "\n",
    "    if 'holder' in metadata:\n",
    "        header_vals[ 'holder' ] = int( get_holder_from_file_name( file ) )\n",
    "        \n",
    "    if 'program' in metadata:\n",
    "        header_vals[ 'program' ] = int( get_program_from_file_name( file ) )\n",
    "\n",
    "    return header_vals\n",
    "\n",
    "\n",
    "def create_column_index( df, metadata, file, channel_prefix = 'C' ):\n",
    "    \"\"\"\n",
    "    Creates a MultiIndex containing metadata values for a single data file.\n",
    "    \n",
    "    :param df: The DataFrame that will use the MultiIndex.\n",
    "    :param metadata: A list of metadata keyword to use, or name-pattern dictionary pairs.\n",
    "    :param file: The file name containing the data.\n",
    "    :param channel_prefix: The prefix for matching channel metadata. [Defualt: 'C']\n",
    "    :returns: A MultiIndex representing the file metadata.\n",
    "    \"\"\"\n",
    "    metrics = df.columns.values\n",
    "        \n",
    "    header_names = metadata.copy()\n",
    "    header_names.append( 'metrics' )\n",
    "    header_vals = metadata_from_file_name( file, metadata )\n",
    "    \n",
    "    header = [ [ header_vals[ val ] ] for val in metadata ]\n",
    "    header.append( metrics )\n",
    "    header = pd.MultiIndex.from_product( header, names = header_names )\n",
    "    \n",
    "    return header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_jv_datum( \n",
    "    file, \n",
    "    precision = 1e-4, \n",
    "    reindex = True, \n",
    "    invert_current = True, \n",
    "    ret_bins = False, \n",
    "    metadata = [ 'holder', 'channel' ]\n",
    "):\n",
    "    \"\"\"\n",
    "    Imports JV data from a .use file.\n",
    "    Due to the large file size and small voltage step \n",
    "        resoluion data can be grouped and averaged over.\n",
    "    \n",
    "    :param file: The file name holding the data.\n",
    "    :param precision: The resolution of data bins. \n",
    "        If two voltage measurements agree up to this \n",
    "        precision they will be binned together. \n",
    "        None to keep all measurements.\n",
    "        [Default: 1e-4]\n",
    "    :param reindex: Use votlage as index. [Default: True]\n",
    "    :param invert_current: Flips the sign of current readings. [Default: True]     \n",
    "    :param ret_bins: Return the voltage bins. [Default: False]\n",
    "    :param metadata: A list of keywords of metadata to use for indexing. \n",
    "        Values: [ 'holder', 'channel', 'program' ]\n",
    "        Default: [ 'holder', 'channel' ]\n",
    "    :returns: A Pandas DataFrame representing the data.\n",
    "    :returns: Votage bins, if ret_bins is True\n",
    "    \"\"\"    \n",
    "    # find header lines\n",
    "    header_pattern = 'number of E\\(V\\), I\\(A\\) couples: (\\d+)'\n",
    "    header_lines = 0\n",
    "    with open( file ) as f:\n",
    "        for num, line in enumerate( f ):    \n",
    "            match = re.match( header_pattern, line )\n",
    "            if match is not None:\n",
    "                # found header line\n",
    "                header_lines = num + 1\n",
    "                break\n",
    "      \n",
    "    # read data\n",
    "    df = pd.read_csv( file, names = [ 'voltage', 'current' ], skiprows = header_lines )    \n",
    "    \n",
    "    if invert_current:\n",
    "        df.current *= -1\n",
    "    \n",
    "    # average data\n",
    "    if precision is not None:\n",
    "        v_min = df.voltage.min()\n",
    "        v_max = df.voltage.max()\n",
    "        bins = math.ceil( (v_max - v_min )/ precision )\n",
    "        bins = np.linspace( v_min, v_max, bins  )\n",
    "        \n",
    "        cuts = pd.cut( df.voltage, bins, include_lowest = True )\n",
    "        df = df.groupby( cuts, as_index = False ).mean()\n",
    "        df = df.dropna( how = 'all' ) # remove empty bins\n",
    "        \n",
    "        # remove insignificant figures\n",
    "        insig = resolution/ 10            \n",
    "        df.index = pd.IntervalIndex( df.index ).mid\n",
    "        df.index = pd.Index( \n",
    "            df.index.to_series().apply( lambda x: np.round( x, int( -np.log10( insig ) ) ) ),\n",
    "            name = 'time'\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        bins = None\n",
    "        \n",
    "    # reindex\n",
    "    if reindex:\n",
    "        df = df.set_index( 'voltage' )\n",
    "        \n",
    "    # metadata\n",
    "    if metadata is not None:\n",
    "        if type( metadata ) is list:\n",
    "            cci = create_column_index # needed for encapsulation\n",
    "            header = cci( df, metadata, file, channel_prefix = channel_prefix )    \n",
    "            \n",
    "        else:\n",
    "            header = std.get_metadata_values( file, metadata, channel_prefix = channel_prefix )\n",
    "            header = pd.MultiIndex( header )\n",
    "            \n",
    "        df.columns = header\n",
    "    \n",
    "    if ret_bins:\n",
    "        return ( df, bins )\n",
    "    \n",
    "    else:\n",
    "        return df\n",
    "    \n",
    "    \n",
    "def import_datum( \n",
    "    file, \n",
    "    use_cols = [ 'time', 'voltage' ], \n",
    "    reindex = None,\n",
    "    metadata = [ 'holder', 'channel' ],\n",
    "    channel_prefix = 'C'\n",
    "):\n",
    "    \"\"\"\n",
    "    Imports measurement data from a .mpt file.\n",
    "    Due to the large file size and small voltage step \n",
    "        resoluion data can be grouped and averaged over.\n",
    "    \n",
    "    :param file: The file name holding the data.\n",
    "    :param use_cols: The name of the columns to include in the dataset.\n",
    "        Values are: [ 'mode', 'ox/red', 'error', 'control changes', 'Ns changes',\n",
    "        'counter inc', 'time', 'control',  'voltage', 'current',  'dq', 'dQ',\n",
    "        '(Q-Qo)', 'Q charge/discharge/', 'half cycle', 'energy charge',\n",
    "        'energy discharge', 'capacitance charge', 'capacitance discharge',\n",
    "        'Q discharge', 'Q charge', 'capacity', 'efficiency', 'cycle number',\n",
    "        'P/W' ]\n",
    "    :param reindex: The column name to use as an index. Values are the \n",
    "        same as in use_cols, and must be included in use_cols. \n",
    "        [Default: None]\n",
    "    :param metadata: A list of keywords of metadata to use for indexing. \n",
    "        Values: [ 'holder', 'channel', 'program' ]\n",
    "        Default: [ 'holder', 'channel' ]\n",
    "    :param channel_prefix: Prefix indicating the channel in the filename. [Default: 'C']\n",
    "    :returns: A Pandas DataFrame representing the data.\n",
    "    :raises RuntimeError: If an invalid column is included in use_cols.\n",
    "    :raises RunTimeError: If reindex column is not included in use_cols.\n",
    "    \"\"\"\n",
    "    encoding = 'unicode_escape'\n",
    "    \n",
    "    # find header lines\n",
    "    header_pattern = 'Nb header lines : (\\d+)'\n",
    "    header_lines = None\n",
    "    with open( file, encoding = encoding ) as f:\n",
    "        try:\n",
    "            for line in f:    \n",
    "                match = re.match( header_pattern, line )\n",
    "                if match is not None:\n",
    "                    # found header line\n",
    "                    header_lines = int( match.group( 1 ) )\n",
    "                    break\n",
    "                    \n",
    "        except Exception:\n",
    "            print( 'Error in {}'.format( file ) )\n",
    "            raise\n",
    "      \n",
    "    # read data\n",
    "    columns = OrderedDict( {\n",
    "        'mode':                     'mode',\n",
    "        'ox/red':                   'ox/red',\n",
    "        'error':                    'error',\n",
    "        'control changes':          'control changes',\n",
    "        'Ns changes':               'Ns changes',\n",
    "        'counter inc.':             'counter inc',\n",
    "        'time/s':                   'time',\n",
    "        'control/mA':               'control',\n",
    "        'control/V':                'control',\n",
    "        'Ewe/V':                    'voltage',\n",
    "        '<Ewe>/V':                  'voltage',\n",
    "        'I/mA':                     'current',\n",
    "        '<I>/mA':                   'current',\n",
    "        'dq/mA.h':                  'dq',\n",
    "        'dQ/mA.h':                  'dQ',\n",
    "        '(Q-Qo)/mA.h':              '(Q-Qo)',\n",
    "        '(Q-Qo)/C':                 '(Q-Qo)',\n",
    "        'Q charge/discharge/mA.h':  'Q charge/discharge/',\n",
    "        'half cycle':               'half cycle',\n",
    "        'Energy charge/W.h':        'energy charge',\n",
    "        'Energy discharge/W.h':     'energy discharge',\n",
    "        'Capacitance charge/µF':    'capacitance charge',\n",
    "        'Capacitance discharge/µF': 'capacitance discharge',\n",
    "        'Q discharge/mA.h':         'Q discharge',\n",
    "        'Q charge/mA.h':            'Q charge',\n",
    "        'Capacity/mA.h':            'capacity',\n",
    "        'Efficiency/%':             'efficiency',\n",
    "        'cycle number':             'cycle number',\n",
    "        'P/W':                      'P/W'\n",
    "    } )\n",
    "    \n",
    "    \n",
    "    # check all use_cols are valid\n",
    "    for col in use_cols:\n",
    "        if not col in columns.values():\n",
    "            raise RuntimeError( 'Invalid column {}.'.format( col ) )\n",
    "            \n",
    "    # get headers in file\n",
    "    with open( file, encoding = encoding ) as f:\n",
    "        lines = f.readlines()\n",
    "        headers = lines[ header_lines - 1 ].strip().split( '\\t' )\n",
    "        \n",
    "    names = [ columns[ header ] for header in headers ]\n",
    "    \n",
    "    df = pd.read_csv( \n",
    "        file, \n",
    "        sep = '\\t', \n",
    "        names = names, \n",
    "        skiprows = header_lines, \n",
    "        usecols = use_cols \n",
    "    )    \n",
    "    \n",
    "    if reindex is not None:\n",
    "        if reindex in use_cols:\n",
    "            df = df.set_index( reindex )\n",
    "            \n",
    "        else:\n",
    "            # reindex column not included in data\n",
    "            raise RuntimeError( 'Invalid index value {}, must be included in columns.'.format( reindex ) )\n",
    "    \n",
    "    # metadata\n",
    "    if metadata is not None:\n",
    "        if type( metadata ) is list:\n",
    "            cci = create_column_index # needed for encapsulation\n",
    "            header = cci( df, metadata, file, channel_prefix = channel_prefix )    \n",
    "            \n",
    "        else:\n",
    "            header = std.get_metadata_values( file, metadata, channel_prefix = channel_prefix )\n",
    "            header = pd.MultiIndex( header )\n",
    "            \n",
    "        df.columns = header\n",
    "    \n",
    "    return df  \n",
    "\n",
    "        \n",
    "        \n",
    "def import_jv_data( \n",
    "    folder_path, \n",
    "    file_pattern = '*.use', \n",
    "    interpolate = 'linear', \n",
    "    fillna = np.nan,\n",
    "    reindex = 'voltage',\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Imports JV data from EC Lab .use output files.\n",
    "    \n",
    "    :param folder_path: The file path containing the data files.\n",
    "    :param file_pattern: A glob pattern to filter the imported files [Default: '*.use']\n",
    "    :param interpolate: How to interpolate data for a common index [Default: linear]\n",
    "        Use None to prevent reindexing\n",
    "    :param fillna: Value to fill NaN values with [Default: np.nan]\n",
    "    :param kwargs: Additional parameters to pass to import_datum.\n",
    "    :returns: A Pandas DataFrame with MultiIndexed columns\n",
    "    :raises RuntimeError: if no files are found\n",
    "    \"\"\"\n",
    "    df = std.import_data( \n",
    "        import_jv_datum, \n",
    "        folder_path, \n",
    "        file_pattern = file_pattern, \n",
    "        interpolate = interpolate, \n",
    "        fillna = fillna,\n",
    "        reindex = reindex,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    return df.sort_index( axis = 1 )\n",
    "    \n",
    "    \n",
    "def import_data( \n",
    "    folder_path, \n",
    "    file_pattern = '*.mpt', \n",
    "    interpolate = 'linear', \n",
    "    fillna = np.nan,\n",
    "    programs = False,\n",
    "    resolution = None,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Imports measurement data from EC Lab .mpt output files.\n",
    "    \n",
    "    :param folder_path: The file path containing the data files.\n",
    "    :param file_pattern: A glob pattern to filter the imported files [Default: '*.use']\n",
    "    :param interpolate: How to interpolate data for a common index [Default: linear]\n",
    "        Use None to prevent reindexing\n",
    "    :param fillna: Value to fill NaN values with [Default: np.nan]\n",
    "    :param programs: Concatenates programs. [Default: False] \n",
    "    :param resolution: The time resolution to use, in same units as measured time (usually seconds). \n",
    "        If None, does not change time step. [Default: None]\n",
    "    :param kwargs: Additional parameters to pass to import_datum.\n",
    "    :returns: A Pandas DataFrame with MultiIndexed columns\n",
    "    :raises RuntimeError: if no files are found\n",
    "    \"\"\"\n",
    "    if programs:\n",
    "        if 'metadata' in kwargs:\n",
    "            if 'program' not in kwargs[ 'metadata' ]:\n",
    "                kwargs[ 'metadata' ].append( 'program' )\n",
    "            \n",
    "        else:\n",
    "            kwargs[ 'metadata' ] = [ 'program' ]\n",
    "    \n",
    "    df = std.import_data( \n",
    "        import_datum, \n",
    "        folder_path, \n",
    "        file_pattern = file_pattern, \n",
    "        interpolate = None, \n",
    "        fillna = fillna,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # merge programs\n",
    "    if programs:\n",
    "        tdf = []\n",
    "        \n",
    "        # group data by levels above program\n",
    "        program_level = df.columns.names.index( 'program' )\n",
    "        group_levels = list( range( program_level ) )\n",
    "        program_groups = (\n",
    "            df.groupby( level = group_levels, axis = 1 ) \n",
    "            if len( group_levels ) > 0\n",
    "            else [ ( '', df ) ]\n",
    "        )\n",
    "        \n",
    "        for name, data in program_groups:\n",
    "            # for each group of programs concatenate them along the 0 axis\n",
    "            pdf = []\n",
    "            time_header = ( *name, 'time' ) if ( type( name ) is tuple ) else 'time' \n",
    "    \n",
    "            for pname, pdata in data.groupby( level = 'program', axis = 1 ):\n",
    "                pdata.columns = pdata.columns.droplevel( 'program' )\n",
    "                pdata = pdata.dropna()\n",
    "                pdata.set_index( time_header, inplace = True )\n",
    "                pdata.index = pdata.index.rename( 'time' )\n",
    "                pdf.append( pdata )\n",
    "\n",
    "            pdf = pd.concat( pdf )    \n",
    "            tdf.append( pdf )\n",
    "        \n",
    "        df = pd.concat( tdf, axis = 1 ).interpolate( interpolate ).sort_index( axis = 1 )\n",
    "    \n",
    "        # average data\n",
    "        if resolution is not None:\n",
    "            t_min = df.index.min()\n",
    "            t_max = df.index.max()\n",
    "            bins = math.ceil( (t_max - t_min )/ resolution )\n",
    "            bins = np.linspace( t_min, t_max, bins  )\n",
    "\n",
    "            times = df.index.to_series()\n",
    "            cuts = pd.cut( times, bins, include_lowest = True )\n",
    "            df = df.groupby( cuts, as_index = True ).mean()\n",
    "            df = df.dropna( how = 'all' ) # remove empty bins\n",
    "\n",
    "            # remove insignificant figures\n",
    "            insig = resolution/ 10            \n",
    "            df.index = pd.IntervalIndex( df.index ).mid\n",
    "            df.index = pd.Index( \n",
    "                df.index.to_series().apply( lambda x: np.round( x, int( -np.log10( insig ) ) ) ),\n",
    "                name = 'time'\n",
    "            )\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_bifurcate( df, threshold = 1e-2, divs = 10 ):\n",
    "    \"\"\"\n",
    "    Takes all data above the first thresholded values.\n",
    "    \n",
    "    :param df: The DataFrame to calculate a threshold value for.\n",
    "    :param threshold: Threshold value relative to most populated state. [Default: 0.01]\n",
    "    :param divs: Number of divisions in the histogram. [Default: 10]\n",
    "    \"\"\"\n",
    "    ( counts, edges ) = np.histogram( df, divs )\n",
    "    counts = counts/ counts.max()\n",
    "    centers = np.mean( [ edges[ 1: ], edges[ :-1 ] ], axis = 0 )\n",
    "    \n",
    "    # mask values below, find indices below first values above threshold\n",
    "    below = np.where( counts < threshold )[ 0 ]\n",
    "    \n",
    "    if ( divs - 1 ) in below:\n",
    "        # remove indices at end\n",
    "        index = 0\n",
    "        cont = True\n",
    "        prev = divs\n",
    "        while cont:\n",
    "            index -= 1\n",
    "            cont = ( below[ index ] == prev - 1 ) # continuous\n",
    "            \n",
    "        below = below[ :index ]\n",
    "        \n",
    "    # no data exceeding threshold\n",
    "    if len( below ) == 0:\n",
    "        logging.warning( 'No data exceeding threshold.' )\n",
    "        return None\n",
    "    \n",
    "    return centers[ below[ -1 ] ]\n",
    "\n",
    "\n",
    "def threshold( df, threshold = density_bifurcate ):\n",
    "    \"\"\"\n",
    "    Thresholds a data set, keeping values above the threshold.\n",
    "    \n",
    "    :param df: The DataFrame to threshold.\n",
    "    :param threshold: A function to calculate the threshold values. [Default: density_bifurcate]\n",
    "    :returns: A new DataFrame with all values below threshold removed.\n",
    "    \"\"\"\n",
    "    # iterate over levels except metrics\n",
    "    groups = data.columns.names\n",
    "    groups = groups[ :-1 ] if ( 'metrics' in groups ) else groups\n",
    "    \n",
    "    td = []\n",
    "    for name, data in df.groupby( level = groups, axis = 1 ):\n",
    "        data = data.copy()\n",
    "        voltage = data.xs( 'voltage', axis = 1, level = 'metrics', drop_level = False )\n",
    "\n",
    "        threshold = threshold( voltage.dropna().values, 1e-3 )\n",
    "        td_th[ name ] = threshold\n",
    "        voltage = voltage.where( voltage > threshold )  \n",
    "        data.loc[ :, ( *name, 'voltage' ) ] = voltage.values.reshape( voltage.shape[ 0 ] )\n",
    "        td.append( data )\n",
    "\n",
    "    td = pd.concat( td, axis = 1 )\n",
    "    return td\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cycle_data( df, split_time = 10, interpolate = 'linear' ):\n",
    "    \"\"\"\n",
    "    Split data into cycles if a time break of longer than split_time occurs\n",
    "    \"\"\" \n",
    "    names = ( df.columns.names \n",
    "             if type( df ) is pd.DataFrame\n",
    "             else [] )\n",
    "             \n",
    "    df = df.dropna()\n",
    "    \n",
    "    # remove extra levels\n",
    "    if type( df.columns ) is pd.MultiIndex:\n",
    "        levels = len( df.columns.levels )\n",
    "        if levels > 1:\n",
    "            df.columns = df.columns.droplevel( list( range( levels - 1 ) ) )\n",
    "        \n",
    "    breaks = np.where( np.diff( df.index.values ) > split_time )[ 0 ] # non continuous index values, before break\n",
    "    \n",
    "    cycles = []\n",
    "    prev = 0\n",
    "    for i in range( len( breaks ) ):\n",
    "        ib = breaks[ i ] + 1; \n",
    "        cycle = df.iloc[ prev:ib ].reset_index()\n",
    "        min_time = cycle.iloc[ 0 ].time\n",
    "        cycle = cycle.assign( rel_time = lambda x: x.time - min_time )\n",
    "        \n",
    "        cycle.columns = pd.MultiIndex.from_product( \n",
    "            [ [ channel ], [ i ], [ 'time', 'voltage', 'rel_time' ] ], \n",
    "            names = names\n",
    "        )\n",
    "        \n",
    "        cycle.index = pd.Float64Index( cycle.index )\n",
    "        cycles.append( cycle )\n",
    "        prev = ib\n",
    "        \n",
    "    return pd.concat( cycles, axis = 1 )\n",
    "\n",
    "\n",
    "        \n",
    "def split_cycles( df, split_time = 10, level = 'channel' ):\n",
    "    \"\"\"\n",
    "    Splits data by cycle.\n",
    "    \"\"\"\n",
    "    groups = ( df.groupby( level = level, axis = 1 )\n",
    "        if ( type( df.columns ) is pd.MultiIndex )\n",
    "        else df.items() )\n",
    "    \n",
    "    split = []\n",
    "    for name, data in groups:\n",
    "        sdf = split_cycle_data( data )\n",
    "        split.append( sdf )\n",
    "        \n",
    "    return pd.concat( split, axis = 1 ).sort_index( axis = 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_threshold( data, div = 'slope', threshold = -1, calc = 'error' ):\n",
    "    \"\"\"\n",
    "    Thresholds data based on the local curvature.\n",
    "    \n",
    "    :param data:\n",
    "    :param div: The type of derivative to examine. Use 'slope' or 'curvature'. [Default: slope]\n",
    "    :param threshold: [Default: -1]\n",
    "    :param calc: The type of threshold to use. \n",
    "        Use 'absolute' or 'error'. [Default: error]\n",
    "    :returns: Thresholded data\n",
    "    \"\"\"\n",
    "    diffs = data.diff()\n",
    "    metric_level = data.columns.names.index( 'metrics' )\n",
    "    for i in range( metric_level ):\n",
    "        diffs.columns = diffs.columns.droplevel( 0 )\n",
    "    \n",
    "    grads = diffs.voltage/ diffs.rel_time\n",
    "    \n",
    "    if div == 'curvature':\n",
    "        diffs = diffs.diff()\n",
    "        grads = diffs.voltage/ diffs.rel_time\n",
    "    \n",
    "    if calc == 'error':\n",
    "        # compute error threshold\n",
    "        threshold = threshold* grads.rolling( window = 5 ).mean().abs()\n",
    "        grads = grads.abs()\n",
    "        \n",
    "        data = data.where( grads < threshold )\n",
    "    \n",
    "    else:\n",
    "        # absolute threshold\n",
    "        if threshold > 0:\n",
    "            data = data.where( grads < threshold )\n",
    "\n",
    "        if threshold < 0:\n",
    "            data = data.where( grads > threshold )\n",
    "        \n",
    "    data = data.dropna()\n",
    "    data = data.reset_index( drop = True )\n",
    "    return data\n",
    "\n",
    "\n",
    "def clean_gradients( df, threshold = 1 ):\n",
    "    clean = []\n",
    "    group_levels = list( range( df.columns.names.index( 'metrics' ) ) )\n",
    "    for name, data in df.groupby( level = group_levels, axis = 1 ):\n",
    "        cd = gradient_threshold( data, threshold = threshold, calc = 'error' ).dropna()\n",
    "        if cd.shape[ 0 ] > 0:\n",
    "            # only append columns with valid voltage data\n",
    "            clean.append( cd )\n",
    "        \n",
    "    return pd.concat( clean, axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_path = 'data/temp/holder-01/ch1/'\n",
    "# jv_path = 'data/holder-01/ch1/1-1-1-3temp-dep_06_CV_C16.use'\n",
    "# df = import_data( data_path, programs = True )\n",
    "# jv_df = import_jv_datum( jv_path, precision = 1e-4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
